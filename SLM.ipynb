{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CLf5xTLyOpSl"
   },
   "outputs": [],
   "source": [
    "with open('abstract.txt', 'r') as f:\n",
    "    ds = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UcQxmdSJQYx0",
    "outputId": "e6958ba7-913e-460a-befb-e6927b9dd7a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To face the global sustainability issues arising from rapid industry\n",
      "development and population increase, many efforts have been made to develop new\n",
      "materials and technologies toward renewable energy and environmental improvement.\n",
      "Zeolites are a family of crystalline materials with orderly distributed micropores\n",
      "in molecular dimensions. As the most important solid catalysts used in traditional\n",
      "petrochemical industries, zeolites are also finding promising applications in many\n",
      "sustainable processes given their unique shape selectivity, adsorption and ion-exchange\n",
      "capability, high hydrothermal stability, tunable acidity and polarity, and low production costs.\n",
      "In this review, we present the state-of-the-art applications of zeolites as potential solutions\n",
      "to the sustainability issues, including biomass conversion, fuel cells, thermal energy storage,\n",
      "CO2 capture and conversion, air-pollution remediation, and water purification, etc.\n",
      "This review highlights recent developments in the synthesis and unconventional applications\n",
      "of nanosized microporous crystals including framework (zeolites) and layered (clays) type\n",
      "materials. Owing to their microporous nature nanosized zeolites and clays exhibit novel\n",
      "properties, different from those of bulk materials. The factors controlling the formation\n",
      "of nanosized microporous crystals are first revised. The most promising approaches from the\n",
      "viewpoint of large-scale production of nanosized zeolites and clays are discussed in depth.\n",
      "The preparation and advanced applications of nanosized zeolites and clays in free (suspension\n",
      "and powder forms) and fixed (films) forms are summarized. Further the review emphasises\n",
      "the non-conventional applications of new porous materials. A comprehensive analysis of the\n",
      "emerging applications of microporous nanosized crystals in the field of semiconductor industry,\n",
      "optical materials, chemical sensors, medicine, cosmetics, and food industry is presented.\n",
      "Finally, the future needs and perspectives of nanosized microporous materials (zeolites\n",
      "and clays) are addressed.\n",
      "Zeolites, owing to their great variety and complexity in structure and wide applications\n",
      "in chemistry, have long been the hot topic in chemical research. This perspective first\n",
      "presents a short retrospect of theoretical investigations on zeolites using the tools from\n",
      "classical force fields to quantum mechanics calculations and to the latest machine learning\n",
      "(ML) potential simulations. ML potentials as the next- generation technique for atomic\n",
      "simulation open new avenues to simulate and interpret zeolite systems and thus hold great\n",
      "promise for finally predicting the structure–functionality relation of zeolites. Recent\n",
      "advances using ML potentials are then summarized from two main aspects: the origin of\n",
      "zeolite stability and the mechanism of zeolite-related catalytic reactions.\n",
      "We also discussed the possible scenarios of ML potential application aiming to provide\n",
      "instantaneous and easy access of zeolite properties. These advanced applications could\n",
      "now be accomplished by combining cloud-computing-based techniques with ML potential-based\n",
      "atomic simulations. The future development of ML potentials for zeolites in the respects\n",
      "of improving the calculation accuracy, expanding the application scope and constructing\n",
      "the zeolite-related datasets is finally outlooked.\n",
      "Zeolites are inorganic materials known for their diversity of applications,\n",
      "synthesis conditions, and resulting polymorphs. Although their synthesis is\n",
      "controlled both by inorganic and organic synthesis conditions, computational\n",
      "studies of zeolite synthesis have focused mostly on the design of organic\n",
      "structure- directing agents (OSDAs). In this work, we combine distances between\n",
      "crystal structures and machine learning (ML) to create inorganic synthesis maps in zeolites.\n",
      "Starting with 253 known zeolites, we show how the continuous distances between frameworks\n",
      "reproduce inorganic synthesis conditions from the literature without using labels such as\n",
      "building units. An unsupervised learning analysis shows that neighboring zeolites according\n",
      "to two different representations often share similar inorganic synthesis conditions, even in\n",
      "OSDA-based routes. In combination with ML classifiers, we find synthesis-structure relationships\n",
      "for 14 common inorganic conditions in zeolites, namely Al, B, Be, Ca, Co, F, Ga, Ge, K, Mg, Na,\n",
      "P, Si, and Zn. By explaining the model predictions, we demonstrate how (dis)similarities towards\n",
      "known structures can be used as features for the synthesis space, thus quantifying the intuition\n",
      "that similar structures often share inorganic synthesis routes. Finally, we show how these\n",
      "methods can be used to predict inorganic synthesis conditions for unrealized frameworks in\n",
      "hypothetical databases and interpret the outcomes by extracting local structural patterns\n",
      "from zeolites. In combination with OSDA design, this work can accelerate the exploration of\n",
      "the space of synthesis conditions for zeolites.\n",
      "We present a database of computationally predicted zeolite-like materials.\n",
      "The materials were identified by a Monte Carlo search of Si atom positions as\n",
      "the number of unique atoms, density, space group, and unit cell of the crystalline\n",
      "material was systematically explored. Over 2.7M unique structures were identified,\n",
      "with roughly 10% within the +30 kJ/mol Si energetic band above R-quartz in which the known\n",
      "zeolites lie. Predicted structures within this band have geometric and topological\n",
      "characteristics similar to that of the known zeolites. Known zeolites are shown to lie\n",
      "on the low-density edge of the distribution of predicted structures. Dielectric constants\n",
      "and X-ray powder diffraction patterns are calculated. Strategies for chemical synthesis of\n",
      "these materials are discussed, a low-density subset of the materials is identified as\n",
      "particularly interesting, and the complementarity of these materials to high-throughput\n",
      "methods is discussed. These structures have been deposited in two publicly available databases.\n",
      "Zeolites are porous, aluminosilicate materials with\n",
      "many industrial and “green” applications. Despite their industrial\n",
      "relevance, many aspects of zeolite synthesis remain poorly\n",
      "understood requiring costly trial and error synthesis. In this\n",
      "paper, we create natural language processing techniques and text\n",
      "markup parsing tools to automatically extract synthesis informa-\n",
      "tion and trends from zeolite journal articles. We further engineer a\n",
      "data set of germanium-containing zeolites to test the accuracy of\n",
      "the extracted data and to discover potential opportunities for\n",
      "zeolites containing germanium. We also create a regression model for a\n",
      "zeolite’s framework density from the synthesis conditions.\n",
      "This model has a cross-validated root mean squared error of 0.98 T/1000 Å3,\n",
      "and many of the model decision boundaries correspond to known synthesis heuristics in\n",
      "germanium-containing zeolites. We propose that this automatic data extraction can be applied\n",
      "to many different problems in zeolite synthesis and enable novel zeolite morphologies.\n",
      "This preamble to a well-known television series captures the challenge encountered not only in\n",
      "space travel adventures, but also in the field of porous materials, which aims to control the size,\n",
      "shape and uniformity of the porous space and the atoms and molecules that define it.\n",
      "The past decade has seen significant advances in the ability to fabricate new porous\n",
      "solids with ordered structures from a wide range of different materials.\n",
      "This has resulted in materials with unusual properties and broadened their application\n",
      "range beyond the traditional use as catalysts and adsorbents. In fact, porous materials\n",
      "now seem set to contribute to developments in areas ranging from microelectronics to medical\n",
      "diagnosis.\n",
      "Small differences between the lattice energies of different zeolites suggest that\n",
      "kinetic factors are of major importance in controlling zeolite nucleation.\n",
      "Thus, it is critical to control the nucleation kinetics in order to obtain a\n",
      "desired microporous material. Here, we demonstrate how careful investigation of the\n",
      "very early stages of zeolite crystallization in colloidal systems can provide access\n",
      "to important nanoscale zeolite phases while avoiding the use of expensive organic templates.\n",
      "We report the effective synthesis of ultrasmall (6- to 15-nanometer) crystals of the\n",
      "large-pore zeolite EMT from template-free colloidal precursors at low temperature (30°C)\n",
      "and very high yield.\n",
      "Zeolites, nanoporous aluminosilicates with well-\n",
      "defined porous structures, are versatile materials with applications\n",
      "in catalysis, gas separation, and ion exchange. Hydrothermal\n",
      "synthesis is widely used for zeolite production, offering control\n",
      "over composition, crystallinity, and pore size. However, the\n",
      "intricate interplay of synthesis parameters necessitates a compre-\n",
      "hensive understanding of synthesis−structure relationships to\n",
      "optimize the synthesis process. Hitherto, public zeolite synthesis\n",
      "databases only contain a subset of parameters and are small in\n",
      "scale, comprising up to a few thousand synthesis routes. We\n",
      "present ZeoSyn, a dataset of 23,961 zeolite hydrothermal synthesis\n",
      "routes, encompassing 233 zeolite topologies and 921 organic\n",
      "structure-directing agents (OSDAs). Each synthesis route com-\n",
      "prises comprehensive synthesis parameters: 1) gel composition, 2) reaction conditions, 3)\n",
      "OSDAs, and 4) zeolite products. Using ZeoSyn, we develop a machine learning classifier to\n",
      "predict the resultant zeolite given a synthesis route with >70% accuracy.\n",
      "We employ SHapley Additive exPlanations (SHAP) to uncover key synthesis parameters\n",
      "for >200 zeolite frameworks. We introduce an aggregation approach to extend SHAP\n",
      "to all building units. We demonstrate applications of this approach to phase-selective\n",
      "and intergrowth synthesis. This comprehensive analysis illuminates the synthesis\n",
      "parameters pivotal in driving zeolite crystallization, offering the potential to guide\n",
      "the synthesis of desired zeolites.\n",
      "It is shown that Machine Learning (ML) algorithms can usefully capture the effect of crystallization\n",
      "composition and conditions (inputs) on key micro- structural characteristics (outputs) of\n",
      "faujasite type zeolites (structure types FAU, EMT, and their intergrowths), which are widely\n",
      "used zeolite catalysts and adsorbents. The utility of ML (in particular, Geometric Harmonics)\n",
      "toward learning input-output relationships of interest is demonstrated, and a com- parison\n",
      "with Neural Networks and Gaussian Process Regression, as alternative approaches, is provided.\n",
      "Through ML, synthesis conditions were identified to enhance the Si/Al ratio of high purity\n",
      "FAU zeolite to the hitherto highest level (i.e., Si/Al = 3.5) achieved via direct (not seeded),\n",
      "and organic structure- directing-agent-free synthesis from sodium aluminosilicate sols.\n",
      "The analysis of the ML algorithms’ results offers the insight that reduced Na2O content is\n",
      "key to formulating FAU materials with high Si/Al ratio. An acid catalyst pre- pared by partial\n",
      "ion exchange of the high-Si/Al-ratio FAU (Si/Al = 3.5) exhibits improved proton reactivity\n",
      "(as well as specific activity, per unit mass of cata- lyst) in propane cracking and\n",
      "dehydrogenation compared to the catalyst prepared from the previously reported highest\n",
      "Si/Al ratio (Si/Al = 2.8).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4SuGVNqdReeV",
    "outputId": "925d32c8-01d2-45ab-936e-710bd942e52d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To face the global sustainability issues arising from rapid industry', 'development and population increase, many efforts have been made to develop new', 'materials and technologies toward renewable energy and environmental improvement.', 'Zeolites are a family of crystalline materials with orderly distributed micropores', 'in molecular dimensions. As the most important solid catalysts used in traditional', 'petrochemical industries, zeolites are also finding promising applications in many', 'sustainable processes given their unique shape selectivity, adsorption and ion-exchange', 'capability, high hydrothermal stability, tunable acidity and polarity, and low production costs.', 'In this review, we present the state-of-the-art applications of zeolites as potential solutions', 'to the sustainability issues, including biomass conversion, fuel cells, thermal energy storage,', 'CO2 capture and conversion, air-pollution remediation, and water purification, etc.', 'This review highlights recent developments in the synthesis and unconventional applications', 'of nanosized microporous crystals including framework (zeolites) and layered (clays) type', 'materials. Owing to their microporous nature nanosized zeolites and clays exhibit novel', 'properties, different from those of bulk materials. The factors controlling the formation', 'of nanosized microporous crystals are first revised. The most promising approaches from the', 'viewpoint of large-scale production of nanosized zeolites and clays are discussed in depth.', 'The preparation and advanced applications of nanosized zeolites and clays in free (suspension', 'and powder forms) and fixed (films) forms are summarized. Further the review emphasises', 'the non-conventional applications of new porous materials. A comprehensive analysis of the', 'emerging applications of microporous nanosized crystals in the field of semiconductor industry,', 'optical materials, chemical sensors, medicine, cosmetics, and food industry is presented.', 'Finally, the future needs and perspectives of nanosized microporous materials (zeolites', 'and clays) are addressed.', 'Zeolites, owing to their great variety and complexity in structure and wide applications', 'in chemistry, have long been the hot topic in chemical research. This perspective first', 'presents a short retrospect of theoretical investigations on zeolites using the tools from', 'classical force fields to quantum mechanics calculations and to the latest machine learning', '(ML) potential simulations. ML potentials as the next- generation technique for atomic', 'simulation open new avenues to simulate and interpret zeolite systems and thus hold great', 'promise for finally predicting the structure–functionality relation of zeolites. Recent', 'advances using ML potentials are then summarized from two main aspects: the origin of', 'zeolite stability and the mechanism of zeolite-related catalytic reactions.', 'We also discussed the possible scenarios of ML potential application aiming to provide', 'instantaneous and easy access of zeolite properties. These advanced applications could', 'now be accomplished by combining cloud-computing-based techniques with ML potential-based', 'atomic simulations. The future development of ML potentials for zeolites in the respects', 'of improving the calculation accuracy, expanding the application scope and constructing', 'the zeolite-related datasets is finally outlooked.', 'Zeolites are inorganic materials known for their diversity of applications,', 'synthesis conditions, and resulting polymorphs. Although their synthesis is', 'controlled both by inorganic and organic synthesis conditions, computational', 'studies of zeolite synthesis have focused mostly on the design of organic', 'structure- directing agents (OSDAs). In this work, we combine distances between', 'crystal structures and machine learning (ML) to create inorganic synthesis maps in zeolites.', 'Starting with 253 known zeolites, we show how the continuous distances between frameworks', 'reproduce inorganic synthesis conditions from the literature without using labels such as', 'building units. An unsupervised learning analysis shows that neighboring zeolites according', 'to two different representations often share similar inorganic synthesis conditions, even in', 'OSDA-based routes. In combination with ML classifiers, we find synthesis-structure relationships', 'for 14 common inorganic conditions in zeolites, namely Al, B, Be, Ca, Co, F, Ga, Ge, K, Mg, Na,', 'P, Si, and Zn. By explaining the model predictions, we demonstrate how (dis)similarities towards', 'known structures can be used as features for the synthesis space, thus quantifying the intuition', 'that similar structures often share inorganic synthesis routes. Finally, we show how these', 'methods can be used to predict inorganic synthesis conditions for unrealized frameworks in', 'hypothetical databases and interpret the outcomes by extracting local structural patterns', 'from zeolites. In combination with OSDA design, this work can accelerate the exploration of', 'the space of synthesis conditions for zeolites.', 'We present a database of computationally predicted zeolite-like materials.', 'The materials were identified by a Monte Carlo search of Si atom positions as', 'the number of unique atoms, density, space group, and unit cell of the crystalline', 'material was systematically explored. Over 2.7M unique structures were identified,', 'with roughly 10% within the +30 kJ/mol Si energetic band above R-quartz in which the known', 'zeolites lie. Predicted structures within this band have geometric and topological', 'characteristics similar to that of the known zeolites. Known zeolites are shown to lie', 'on the low-density edge of the distribution of predicted structures. Dielectric constants', 'and X-ray powder diffraction patterns are calculated. Strategies for chemical synthesis of', 'these materials are discussed, a low-density subset of the materials is identified as', 'particularly interesting, and the complementarity of these materials to high-throughput', 'methods is discussed. These structures have been deposited in two publicly available databases.', 'Zeolites are porous, aluminosilicate materials with', 'many industrial and “green” applications. Despite their industrial', 'relevance, many aspects of zeolite synthesis remain poorly', 'understood requiring costly trial and error synthesis. In this', 'paper, we create natural language processing techniques and text', 'markup parsing tools to automatically extract synthesis informa-', 'tion and trends from zeolite journal articles. We further engineer a', 'data set of germanium-containing zeolites to test the accuracy of', 'the extracted data and to discover potential opportunities for', 'zeolites containing germanium. We also create a regression model for a', 'zeolite’s framework density from the synthesis conditions.', 'This model has a cross-validated root mean squared error of 0.98 T/1000 Å3,', 'and many of the model decision boundaries correspond to known synthesis heuristics in', 'germanium-containing zeolites. We propose that this automatic data extraction can be applied', 'to many different problems in zeolite synthesis and enable novel zeolite morphologies.', 'This preamble to a well-known television series captures the challenge encountered not only in', 'space travel adventures, but also in the field of porous materials, which aims to control the size,', 'shape and uniformity of the porous space and the atoms and molecules that define it.', 'The past decade has seen significant advances in the ability to fabricate new porous', 'solids with ordered structures from a wide range of different materials.', 'This has resulted in materials with unusual properties and broadened their application', 'range beyond the traditional use as catalysts and adsorbents. In fact, porous materials', 'now seem set to contribute to developments in areas ranging from microelectronics to medical', 'diagnosis.', 'Small differences between the lattice energies of different zeolites suggest that', 'kinetic factors are of major importance in controlling zeolite nucleation.', 'Thus, it is critical to control the nucleation kinetics in order to obtain a', 'desired microporous material. Here, we demonstrate how careful investigation of the', 'very early stages of zeolite crystallization in colloidal systems can provide access', 'to important nanoscale zeolite phases while avoiding the use of expensive organic templates.', 'We report the effective synthesis of ultrasmall (6- to 15-nanometer) crystals of the', 'large-pore zeolite EMT from template-free colloidal precursors at low temperature (30°C)', 'and very high yield.', 'Zeolites, nanoporous aluminosilicates with well-', 'defined porous structures, are versatile materials with applications', 'in catalysis, gas separation, and ion exchange. Hydrothermal', 'synthesis is widely used for zeolite production, offering control', 'over composition, crystallinity, and pore size. However, the', 'intricate interplay of synthesis parameters necessitates a compre-', 'hensive understanding of synthesis−structure relationships to', 'optimize the synthesis process. Hitherto, public zeolite synthesis', 'databases only contain a subset of parameters and are small in', 'scale, comprising up to a few thousand synthesis routes. We', 'present ZeoSyn, a dataset of 23,961 zeolite hydrothermal synthesis', 'routes, encompassing 233 zeolite topologies and 921 organic', 'structure-directing agents (OSDAs). Each synthesis route com-', 'prises comprehensive synthesis parameters: 1) gel composition, 2) reaction conditions, 3)', 'OSDAs, and 4) zeolite products. Using ZeoSyn, we develop a machine learning classifier to', 'predict the resultant zeolite given a synthesis route with >70% accuracy.', 'We employ SHapley Additive exPlanations (SHAP) to uncover key synthesis parameters', 'for >200 zeolite frameworks. We introduce an aggregation approach to extend SHAP', 'to all building units. We demonstrate applications of this approach to phase-selective', 'and intergrowth synthesis. This comprehensive analysis illuminates the synthesis', 'parameters pivotal in driving zeolite crystallization, offering the potential to guide', 'the synthesis of desired zeolites.', 'It is shown that Machine Learning (ML) algorithms can usefully capture the effect of crystallization', 'composition and conditions (inputs) on key micro- structural characteristics (outputs) of', 'faujasite type zeolites (structure types FAU, EMT, and their intergrowths), which are widely', 'used zeolite catalysts and adsorbents. The utility of ML (in particular, Geometric Harmonics)', 'toward learning input-output relationships of interest is demonstrated, and a com- parison', 'with Neural Networks and Gaussian Process Regression, as alternative approaches, is provided.', 'Through ML, synthesis conditions were identified to enhance the Si/Al ratio of high purity', 'FAU zeolite to the hitherto highest level (i.e., Si/Al = 3.5) achieved via direct (not seeded),', 'and organic structure- directing-agent-free synthesis from sodium aluminosilicate sols.', 'The analysis of the ML algorithms’ results offers the insight that reduced Na2O content is', 'key to formulating FAU materials with high Si/Al ratio. An acid catalyst pre- pared by partial', 'ion exchange of the high-Si/Al-ratio FAU (Si/Al = 3.5) exhibits improved proton reactivity', '(as well as specific activity, per unit mass of cata- lyst) in propane cracking and', 'dehydrogenation compared to the catalyst prepared from the previously reported highest', 'Si/Al ratio (Si/Al = 2.8).']\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Read the whole file\n",
    "with open('abstract.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Step 2: Split into paragraphs (or sentences, if preferred)\n",
    "paragraphs = [p.strip() for p in text.split('\\n') if p.strip()]  # Non-empty lines\n",
    "print(paragraphs)\n",
    "# Step 3: Save in the right format: One example per line\n",
    "with open(\"formatted_abstract.txt\", \"w\") as f:\n",
    "    for para in paragraphs:\n",
    "        f.write(para + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6FgZRnSFSIUS",
    "outputId": "924ae154-5788-44ce-8d1f-a6d8fd8a8ad6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 140\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Re-use your earlier code to read and split text\n",
    "with open('abstract.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "paragraphs = [p.strip() for p in text.split('\\n') if p.strip()]\n",
    "\n",
    "# Create Hugging Face dataset from list\n",
    "dataset = Dataset.from_dict({\"text\": paragraphs})\n",
    "print(dataset)\n",
    "ds=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFFOmHEkQY0H"
   },
   "outputs": [],
   "source": [
    "#tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ABmuH_itQY2X"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def process(example):\n",
    "    ids = enc.encode_ordinary(example['text'])\n",
    "    return {'ids': ids, 'len': len(ids)}\n",
    "\n",
    "if not os.path.exists(\"train.bin\"):\n",
    "    tokenized = ds.map(\n",
    "        process,\n",
    "        remove_columns=['text'],\n",
    "        desc=\"tokenizing the dataset\",\n",
    "        num_proc=8,\n",
    "    )\n",
    "\n",
    "    arr_len = np.sum(tokenized['len'], dtype=np.uint64)\n",
    "    filename = 'train.bin'\n",
    "    dtype = np.uint16\n",
    "\n",
    "    arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len,))\n",
    "    total_batches = 1024\n",
    "\n",
    "    idx = 0\n",
    "    for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "        batch = tokenized.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('python')\n",
    "        arr_batch = np.concatenate(batch['ids'])\n",
    "        arr[idx : idx + len(arr_batch)] = arr_batch\n",
    "        idx += len(arr_batch)\n",
    "\n",
    "    arr.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "TQjCUkNKQY4h"
   },
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "\n",
    "    if split == 'train':\n",
    "        data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap('validation.bin', dtype=np.uint16, mode='r')\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "fUThPCMlQY7K"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from contextlib import nullcontext\n",
    "import os\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "    def forward(self, x):\n",
    "        return F.layer_norm(x, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                       .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "\n",
    "        if self.flash:\n",
    "            y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.attn_dropout.p if self.training else 0.0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.c_proj(self.gelu(self.c_fc(x))))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln2 = LayerNorm(config.n_embd, config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe=nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop=nn.Dropout(config.dropout),\n",
    "            h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f=LayerNorm(config.n_embd, config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "            return logits, loss\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            return logits, None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Generate tokens given a conditioning sequence.\n",
    "        idx: Tensor of shape (B, T)\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "0XEA3WhiQY9H"
   },
   "outputs": [],
   "source": [
    "config = GPTConfig(\n",
    "    vocab_size=50257,     # use the tokenizer's vocab size\n",
    "    block_size=128,       # or whatever context size you're training with\n",
    "    n_layer=6,\n",
    "    n_head=6,\n",
    "    n_embd=384,\n",
    "    dropout=0.1,\n",
    "    bias=True\n",
    ")\n",
    "\n",
    "model = GPT(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "n8NW_FF2QY_N"
   },
   "outputs": [],
   "source": [
    "def estimate_loss(model):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    with torch.inference_mode():\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(\"train\")  # only use train split\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out['train'] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hUBF5hEdQZB0",
    "outputId": "d728b03e-4913-4880-ba20-03c479ae34e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7d6573c171d0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training Config\n",
    "import torch\n",
    "from contextlib import nullcontext\n",
    "\n",
    "learning_rate = 1e-4\n",
    "max_iters = 10000\n",
    "warmup_steps = 1000\n",
    "min_lr = 5e-4\n",
    "eval_iters = 2\n",
    "batch_size = 6\n",
    "block_size = 128\n",
    "\n",
    "gradient_accumulation_steps = 32\n",
    "\n",
    "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu'\n",
    "\n",
    "\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "torch.set_default_device(device)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIgf4aHhQZEX",
    "outputId": "6d5ea72f-8842-4b0a-d562-4f810fd4075f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-89-8b14655fd386>:10: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import LinearLR,SequentialLR, CosineAnnealingLR\n",
    "\n",
    "##PUT IN WEIGHT DECAY, CHANGED BETA2 to 0.95\n",
    "optimizer =  torch.optim.AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=0.1, eps=1e-9)\n",
    "\n",
    "scheduler_warmup = LinearLR(optimizer, total_iters = warmup_steps) #Implement linear warmup\n",
    "scheduler_decay = CosineAnnealingLR(optimizer,T_max = max_iters - warmup_steps, eta_min = min_lr) #Implement lr decay\n",
    "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_decay], milestones=[warmup_steps])\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPZZHwmvQZLs"
   },
   "outputs": [],
   "source": [
    "#pretraing the small language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "f622839e6ebc458c8536174713d96a12",
      "6d7b613e94454059885788cc7fbf7f95",
      "dad97c3adf974344aad23b2f0989852e",
      "a43415a28a1641e39e8b4505bc99a2af",
      "3ca30d2521da4e4b8324eb365636ac3c",
      "b07e24380423450f8197024d063877f6",
      "b44f1305bf8b4c10ae8507989652d9a7",
      "f20ffe8779e04a45910d964632296575",
      "d5a0eb54f7d64563bf4e98920bdaaf78",
      "158b951ececd40d78e0df5cca76d3e0a",
      "7aa046c30428450eaf3a280eaf5246ae"
     ]
    },
    "id": "Jv8iEYCFQZOC",
    "outputId": "b5b65218-b610-4cc5-b4db-67077b1902f3"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f622839e6ebc458c8536174713d96a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "Epoch 5000: train loss 0.0568\n",
      "Learning rate: 0.00002\n",
      "Epoch 5002: train loss 0.0568\n",
      "Learning rate: 0.00002\n",
      "Epoch 5004: train loss 0.0568\n",
      "Learning rate: 0.00002\n",
      "Epoch 5006: train loss 0.0568\n",
      "Learning rate: 0.00002\n",
      "Epoch 5008: train loss 0.0568\n",
      "Learning rate: 0.00002\n",
      "Epoch 5010: train loss 0.0568\n",
      "Learning rate: 0.00002\n",
      "Epoch 5012: train loss 0.0568\n",
      "Learning rate: 0.00002\n",
      "Epoch 5014: train loss 0.0568\n",
      "Learning rate: 0.00002\n",
      "Epoch 5016: train loss 0.0568\n",
      "Learning rate: 0.00002\n",
      "Epoch 5018: train loss 0.0568\n",
      "Learning rate: 0.00002\n",
      "Epoch 5020: train loss 0.0568\n",
      "Learning rate: 0.00002\n",
      "Epoch 5022: train loss 0.0568\n",
      "Learning rate: 0.00002\n",
      "Epoch 5024: train loss 0.0564\n",
      "Learning rate: 0.00002\n",
      "Epoch 5026: train loss 0.0564\n",
      "Learning rate: 0.00002\n",
      "Epoch 5028: train loss 0.0564\n",
      "Learning rate: 0.00002\n",
      "Epoch 5030: train loss 0.0564\n",
      "Learning rate: 0.00002\n",
      "Epoch 5032: train loss 0.0564\n",
      "Learning rate: 0.00002\n",
      "Epoch 5034: train loss 0.0564\n",
      "Learning rate: 0.00002\n",
      "Epoch 5036: train loss 0.0564\n",
      "Learning rate: 0.00002\n",
      "Epoch 5038: train loss 0.0564\n",
      "Learning rate: 0.00002\n",
      "Epoch 5040: train loss 0.0564\n",
      "Learning rate: 0.00002\n",
      "Epoch 5042: train loss 0.0564\n",
      "Learning rate: 0.00002\n",
      "Epoch 5044: train loss 0.0564\n",
      "Learning rate: 0.00002\n",
      "Epoch 5046: train loss 0.0564\n",
      "Learning rate: 0.00002\n",
      "Epoch 5048: train loss 0.0564\n",
      "Learning rate: 0.00002\n",
      "Epoch 5050: train loss 0.0564\n",
      "Learning rate: 0.00002\n",
      "Epoch 5052: train loss 0.0564\n",
      "Learning rate: 0.00002\n",
      "Epoch 5054: train loss 0.0564\n",
      "Learning rate: 0.00002\n",
      "Epoch 5056: train loss 0.0534\n",
      "Learning rate: 0.00002\n",
      "Epoch 5058: train loss 0.0534\n",
      "Learning rate: 0.00002\n",
      "Epoch 5060: train loss 0.0534\n",
      "Learning rate: 0.00002\n",
      "Epoch 5062: train loss 0.0534\n",
      "Learning rate: 0.00002\n",
      "Epoch 5064: train loss 0.0534\n",
      "Learning rate: 0.00002\n",
      "Epoch 5066: train loss 0.0534\n",
      "Learning rate: 0.00002\n",
      "Epoch 5068: train loss 0.0534\n",
      "Learning rate: 0.00002\n",
      "Epoch 5070: train loss 0.0534\n",
      "Learning rate: 0.00002\n",
      "Epoch 5072: train loss 0.0534\n",
      "Learning rate: 0.00002\n",
      "Epoch 5074: train loss 0.0534\n",
      "Learning rate: 0.00002\n",
      "Epoch 5076: train loss 0.0534\n",
      "Learning rate: 0.00002\n",
      "Epoch 5078: train loss 0.0534\n",
      "Learning rate: 0.00002\n",
      "Epoch 5080: train loss 0.0534\n",
      "Learning rate: 0.00002\n",
      "Epoch 5082: train loss 0.0534\n",
      "Learning rate: 0.00002\n",
      "Epoch 5084: train loss 0.0534\n",
      "Learning rate: 0.00002\n",
      "Epoch 5086: train loss 0.0534\n",
      "Learning rate: 0.00002\n",
      "Epoch 5088: train loss 0.0524\n",
      "Learning rate: 0.00002\n",
      "Epoch 5090: train loss 0.0524\n",
      "Learning rate: 0.00002\n",
      "Epoch 5092: train loss 0.0524\n",
      "Learning rate: 0.00002\n",
      "Epoch 5094: train loss 0.0524\n",
      "Learning rate: 0.00002\n",
      "Epoch 5096: train loss 0.0524\n",
      "Learning rate: 0.00002\n",
      "Epoch 5098: train loss 0.0524\n",
      "Learning rate: 0.00002\n",
      "Epoch 5100: train loss 0.0524\n",
      "Learning rate: 0.00002\n",
      "Epoch 5102: train loss 0.0524\n",
      "Learning rate: 0.00002\n",
      "Epoch 5104: train loss 0.0524\n",
      "Learning rate: 0.00002\n",
      "Epoch 5106: train loss 0.0524\n",
      "Learning rate: 0.00002\n",
      "Epoch 5108: train loss 0.0524\n",
      "Learning rate: 0.00002\n",
      "Epoch 5110: train loss 0.0524\n",
      "Learning rate: 0.00002\n",
      "Epoch 5112: train loss 0.0524\n",
      "Learning rate: 0.00002\n",
      "Epoch 5114: train loss 0.0524\n",
      "Learning rate: 0.00002\n",
      "Epoch 5116: train loss 0.0524\n",
      "Learning rate: 0.00002\n",
      "Epoch 5118: train loss 0.0524\n",
      "Learning rate: 0.00002\n",
      "Epoch 5120: train loss 0.0521\n",
      "Learning rate: 0.00002\n",
      "Epoch 5122: train loss 0.0521\n",
      "Learning rate: 0.00002\n",
      "Epoch 5124: train loss 0.0521\n",
      "Learning rate: 0.00002\n",
      "Epoch 5126: train loss 0.0521\n",
      "Learning rate: 0.00002\n",
      "Epoch 5128: train loss 0.0521\n",
      "Learning rate: 0.00002\n",
      "Epoch 5130: train loss 0.0521\n",
      "Learning rate: 0.00002\n",
      "Epoch 5132: train loss 0.0521\n",
      "Learning rate: 0.00002\n",
      "Epoch 5134: train loss 0.0521\n",
      "Learning rate: 0.00002\n",
      "Epoch 5136: train loss 0.0521\n",
      "Learning rate: 0.00002\n",
      "Epoch 5138: train loss 0.0521\n",
      "Learning rate: 0.00002\n",
      "Epoch 5140: train loss 0.0521\n",
      "Learning rate: 0.00002\n",
      "Epoch 5142: train loss 0.0521\n",
      "Learning rate: 0.00002\n",
      "Epoch 5144: train loss 0.0521\n",
      "Learning rate: 0.00002\n",
      "Epoch 5146: train loss 0.0521\n",
      "Learning rate: 0.00002\n",
      "Epoch 5148: train loss 0.0521\n",
      "Learning rate: 0.00002\n",
      "Epoch 5150: train loss 0.0521\n",
      "Learning rate: 0.00002\n",
      "Epoch 5152: train loss 0.0518\n",
      "Learning rate: 0.00002\n",
      "Epoch 5154: train loss 0.0518\n",
      "Learning rate: 0.00002\n",
      "Epoch 5156: train loss 0.0518\n",
      "Learning rate: 0.00002\n",
      "Epoch 5158: train loss 0.0518\n",
      "Learning rate: 0.00002\n",
      "Epoch 5160: train loss 0.0518\n",
      "Learning rate: 0.00002\n",
      "Epoch 5162: train loss 0.0518\n",
      "Learning rate: 0.00002\n",
      "Epoch 5164: train loss 0.0518\n",
      "Learning rate: 0.00002\n",
      "Epoch 5166: train loss 0.0518\n",
      "Learning rate: 0.00002\n",
      "Epoch 5168: train loss 0.0518\n",
      "Learning rate: 0.00002\n",
      "Epoch 5170: train loss 0.0518\n",
      "Learning rate: 0.00002\n",
      "Epoch 5172: train loss 0.0518\n",
      "Learning rate: 0.00002\n",
      "Epoch 5174: train loss 0.0518\n",
      "Learning rate: 0.00002\n",
      "Epoch 5176: train loss 0.0518\n",
      "Learning rate: 0.00002\n",
      "Epoch 5178: train loss 0.0518\n",
      "Learning rate: 0.00002\n",
      "Epoch 5180: train loss 0.0518\n",
      "Learning rate: 0.00002\n",
      "Epoch 5182: train loss 0.0518\n",
      "Learning rate: 0.00002\n",
      "Epoch 5184: train loss 0.0515\n",
      "Learning rate: 0.00002\n",
      "Epoch 5186: train loss 0.0515\n",
      "Learning rate: 0.00002\n",
      "Epoch 5188: train loss 0.0515\n",
      "Learning rate: 0.00002\n",
      "Epoch 5190: train loss 0.0515\n",
      "Learning rate: 0.00002\n",
      "Epoch 5192: train loss 0.0515\n",
      "Learning rate: 0.00002\n",
      "Epoch 5194: train loss 0.0515\n",
      "Learning rate: 0.00002\n",
      "Epoch 5196: train loss 0.0515\n",
      "Learning rate: 0.00002\n",
      "Epoch 5198: train loss 0.0515\n",
      "Learning rate: 0.00002\n",
      "Epoch 5200: train loss 0.0515\n",
      "Learning rate: 0.00002\n",
      "Epoch 5202: train loss 0.0515\n",
      "Learning rate: 0.00002\n",
      "Epoch 5204: train loss 0.0515\n",
      "Learning rate: 0.00002\n",
      "Epoch 5206: train loss 0.0515\n",
      "Learning rate: 0.00002\n",
      "Epoch 5208: train loss 0.0515\n",
      "Learning rate: 0.00002\n",
      "Epoch 5210: train loss 0.0515\n",
      "Learning rate: 0.00002\n",
      "Epoch 5212: train loss 0.0515\n",
      "Learning rate: 0.00002\n",
      "Epoch 5214: train loss 0.0515\n",
      "Learning rate: 0.00002\n",
      "Epoch 5216: train loss 0.0512\n",
      "Learning rate: 0.00002\n",
      "Epoch 5218: train loss 0.0512\n",
      "Learning rate: 0.00002\n",
      "Epoch 5220: train loss 0.0512\n",
      "Learning rate: 0.00002\n",
      "Epoch 5222: train loss 0.0512\n",
      "Learning rate: 0.00002\n",
      "Epoch 5224: train loss 0.0512\n",
      "Learning rate: 0.00002\n",
      "Epoch 5226: train loss 0.0512\n",
      "Learning rate: 0.00002\n",
      "Epoch 5228: train loss 0.0512\n",
      "Learning rate: 0.00002\n",
      "Epoch 5230: train loss 0.0512\n",
      "Learning rate: 0.00002\n",
      "Epoch 5232: train loss 0.0512\n",
      "Learning rate: 0.00002\n",
      "Epoch 5234: train loss 0.0512\n",
      "Learning rate: 0.00002\n",
      "Epoch 5236: train loss 0.0512\n",
      "Learning rate: 0.00002\n",
      "Epoch 5238: train loss 0.0512\n",
      "Learning rate: 0.00002\n",
      "Epoch 5240: train loss 0.0512\n",
      "Learning rate: 0.00002\n",
      "Epoch 5242: train loss 0.0512\n",
      "Learning rate: 0.00002\n",
      "Epoch 5244: train loss 0.0512\n",
      "Learning rate: 0.00002\n",
      "Epoch 5246: train loss 0.0512\n",
      "Learning rate: 0.00002\n",
      "Epoch 5248: train loss 0.0509\n",
      "Learning rate: 0.00002\n",
      "Epoch 5250: train loss 0.0509\n",
      "Learning rate: 0.00002\n",
      "Epoch 5252: train loss 0.0509\n",
      "Learning rate: 0.00002\n",
      "Epoch 5254: train loss 0.0509\n",
      "Learning rate: 0.00002\n",
      "Epoch 5256: train loss 0.0509\n",
      "Learning rate: 0.00002\n",
      "Epoch 5258: train loss 0.0509\n",
      "Learning rate: 0.00002\n",
      "Epoch 5260: train loss 0.0509\n",
      "Learning rate: 0.00002\n",
      "Epoch 5262: train loss 0.0509\n",
      "Learning rate: 0.00002\n",
      "Epoch 5264: train loss 0.0509\n",
      "Learning rate: 0.00002\n",
      "Epoch 5266: train loss 0.0509\n",
      "Learning rate: 0.00002\n",
      "Epoch 5268: train loss 0.0509\n",
      "Learning rate: 0.00002\n",
      "Epoch 5270: train loss 0.0509\n",
      "Learning rate: 0.00002\n",
      "Epoch 5272: train loss 0.0509\n",
      "Learning rate: 0.00002\n",
      "Epoch 5274: train loss 0.0509\n",
      "Learning rate: 0.00002\n",
      "Epoch 5276: train loss 0.0509\n",
      "Learning rate: 0.00002\n",
      "Epoch 5278: train loss 0.0509\n",
      "Learning rate: 0.00002\n",
      "Epoch 5280: train loss 0.0506\n",
      "Learning rate: 0.00002\n",
      "Epoch 5282: train loss 0.0506\n",
      "Learning rate: 0.00002\n",
      "Epoch 5284: train loss 0.0506\n",
      "Learning rate: 0.00002\n",
      "Epoch 5286: train loss 0.0506\n",
      "Learning rate: 0.00002\n",
      "Epoch 5288: train loss 0.0506\n",
      "Learning rate: 0.00002\n",
      "Epoch 5290: train loss 0.0506\n",
      "Learning rate: 0.00002\n",
      "Epoch 5292: train loss 0.0506\n",
      "Learning rate: 0.00002\n",
      "Epoch 5294: train loss 0.0506\n",
      "Learning rate: 0.00002\n",
      "Epoch 5296: train loss 0.0506\n",
      "Learning rate: 0.00002\n",
      "Epoch 5298: train loss 0.0506\n",
      "Learning rate: 0.00002\n",
      "Epoch 5300: train loss 0.0506\n",
      "Learning rate: 0.00002\n",
      "Epoch 5302: train loss 0.0506\n",
      "Learning rate: 0.00002\n",
      "Epoch 5304: train loss 0.0506\n",
      "Learning rate: 0.00002\n",
      "Epoch 5306: train loss 0.0506\n",
      "Learning rate: 0.00002\n",
      "Epoch 5308: train loss 0.0506\n",
      "Learning rate: 0.00002\n",
      "Epoch 5310: train loss 0.0506\n",
      "Learning rate: 0.00002\n",
      "Epoch 5312: train loss 0.0503\n",
      "Learning rate: 0.00002\n",
      "Epoch 5314: train loss 0.0503\n",
      "Learning rate: 0.00002\n",
      "Epoch 5316: train loss 0.0503\n",
      "Learning rate: 0.00002\n",
      "Epoch 5318: train loss 0.0503\n",
      "Learning rate: 0.00002\n",
      "Epoch 5320: train loss 0.0503\n",
      "Learning rate: 0.00001\n",
      "Epoch 5322: train loss 0.0503\n",
      "Learning rate: 0.00001\n",
      "Epoch 5324: train loss 0.0503\n",
      "Learning rate: 0.00001\n",
      "Epoch 5326: train loss 0.0503\n",
      "Learning rate: 0.00001\n",
      "Epoch 5328: train loss 0.0503\n",
      "Learning rate: 0.00001\n",
      "Epoch 5330: train loss 0.0503\n",
      "Learning rate: 0.00001\n",
      "Epoch 5332: train loss 0.0503\n",
      "Learning rate: 0.00001\n",
      "Epoch 5334: train loss 0.0503\n",
      "Learning rate: 0.00001\n",
      "Epoch 5336: train loss 0.0503\n",
      "Learning rate: 0.00001\n",
      "Epoch 5338: train loss 0.0503\n",
      "Learning rate: 0.00001\n",
      "Epoch 5340: train loss 0.0503\n",
      "Learning rate: 0.00001\n",
      "Epoch 5342: train loss 0.0503\n",
      "Learning rate: 0.00001\n",
      "Epoch 5344: train loss 0.0500\n",
      "Learning rate: 0.00001\n",
      "Epoch 5346: train loss 0.0500\n",
      "Learning rate: 0.00001\n",
      "Epoch 5348: train loss 0.0500\n",
      "Learning rate: 0.00001\n",
      "Epoch 5350: train loss 0.0500\n",
      "Learning rate: 0.00001\n",
      "Epoch 5352: train loss 0.0500\n",
      "Learning rate: 0.00001\n",
      "Epoch 5354: train loss 0.0500\n",
      "Learning rate: 0.00001\n",
      "Epoch 5356: train loss 0.0500\n",
      "Learning rate: 0.00001\n",
      "Epoch 5358: train loss 0.0500\n",
      "Learning rate: 0.00001\n",
      "Epoch 5360: train loss 0.0500\n",
      "Learning rate: 0.00001\n",
      "Epoch 5362: train loss 0.0500\n",
      "Learning rate: 0.00001\n",
      "Epoch 5364: train loss 0.0500\n",
      "Learning rate: 0.00001\n",
      "Epoch 5366: train loss 0.0500\n",
      "Learning rate: 0.00001\n",
      "Epoch 5368: train loss 0.0500\n",
      "Learning rate: 0.00001\n",
      "Epoch 5370: train loss 0.0500\n",
      "Learning rate: 0.00001\n",
      "Epoch 5372: train loss 0.0500\n",
      "Learning rate: 0.00001\n",
      "Epoch 5374: train loss 0.0500\n",
      "Learning rate: 0.00001\n",
      "Epoch 5376: train loss 0.0497\n",
      "Learning rate: 0.00001\n",
      "Epoch 5378: train loss 0.0497\n",
      "Learning rate: 0.00001\n",
      "Epoch 5380: train loss 0.0497\n",
      "Learning rate: 0.00001\n",
      "Epoch 5382: train loss 0.0497\n",
      "Learning rate: 0.00001\n",
      "Epoch 5384: train loss 0.0497\n",
      "Learning rate: 0.00001\n",
      "Epoch 5386: train loss 0.0497\n",
      "Learning rate: 0.00001\n",
      "Epoch 5388: train loss 0.0497\n",
      "Learning rate: 0.00001\n",
      "Epoch 5390: train loss 0.0497\n",
      "Learning rate: 0.00001\n",
      "Epoch 5392: train loss 0.0497\n",
      "Learning rate: 0.00001\n",
      "Epoch 5394: train loss 0.0497\n",
      "Learning rate: 0.00001\n",
      "Epoch 5396: train loss 0.0497\n",
      "Learning rate: 0.00001\n",
      "Epoch 5398: train loss 0.0497\n",
      "Learning rate: 0.00001\n",
      "Epoch 5400: train loss 0.0497\n",
      "Learning rate: 0.00001\n",
      "Epoch 5402: train loss 0.0497\n",
      "Learning rate: 0.00001\n",
      "Epoch 5404: train loss 0.0497\n",
      "Learning rate: 0.00001\n",
      "Epoch 5406: train loss 0.0497\n",
      "Learning rate: 0.00001\n",
      "Epoch 5408: train loss 0.0491\n",
      "Learning rate: 0.00001\n",
      "Epoch 5410: train loss 0.0491\n",
      "Learning rate: 0.00001\n",
      "Epoch 5412: train loss 0.0491\n",
      "Learning rate: 0.00001\n",
      "Epoch 5414: train loss 0.0491\n",
      "Learning rate: 0.00001\n",
      "Epoch 5416: train loss 0.0491\n",
      "Learning rate: 0.00001\n",
      "Epoch 5418: train loss 0.0491\n",
      "Learning rate: 0.00001\n",
      "Epoch 5420: train loss 0.0491\n",
      "Learning rate: 0.00001\n",
      "Epoch 5422: train loss 0.0491\n",
      "Learning rate: 0.00001\n",
      "Epoch 5424: train loss 0.0491\n",
      "Learning rate: 0.00001\n",
      "Epoch 5426: train loss 0.0491\n",
      "Learning rate: 0.00001\n",
      "Epoch 5428: train loss 0.0491\n",
      "Learning rate: 0.00001\n",
      "Epoch 5430: train loss 0.0491\n",
      "Learning rate: 0.00001\n",
      "Epoch 5432: train loss 0.0491\n",
      "Learning rate: 0.00001\n",
      "Epoch 5434: train loss 0.0491\n",
      "Learning rate: 0.00001\n",
      "Epoch 5436: train loss 0.0491\n",
      "Learning rate: 0.00001\n",
      "Epoch 5438: train loss 0.0491\n",
      "Learning rate: 0.00001\n",
      "Epoch 5440: train loss 0.0463\n",
      "Learning rate: 0.00001\n",
      "Epoch 5442: train loss 0.0463\n",
      "Learning rate: 0.00001\n",
      "Epoch 5444: train loss 0.0463\n",
      "Learning rate: 0.00001\n",
      "Epoch 5446: train loss 0.0463\n",
      "Learning rate: 0.00001\n",
      "Epoch 5448: train loss 0.0463\n",
      "Learning rate: 0.00001\n",
      "Epoch 5450: train loss 0.0463\n",
      "Learning rate: 0.00001\n",
      "Epoch 5452: train loss 0.0463\n",
      "Learning rate: 0.00001\n",
      "Epoch 5454: train loss 0.0463\n",
      "Learning rate: 0.00001\n",
      "Epoch 5456: train loss 0.0463\n",
      "Learning rate: 0.00001\n",
      "Epoch 5458: train loss 0.0463\n",
      "Learning rate: 0.00001\n",
      "Epoch 5460: train loss 0.0463\n",
      "Learning rate: 0.00001\n",
      "Epoch 5462: train loss 0.0463\n",
      "Learning rate: 0.00001\n",
      "Epoch 5464: train loss 0.0463\n",
      "Learning rate: 0.00001\n",
      "Epoch 5466: train loss 0.0463\n",
      "Learning rate: 0.00001\n",
      "Epoch 5468: train loss 0.0463\n",
      "Learning rate: 0.00001\n",
      "Epoch 5470: train loss 0.0463\n",
      "Learning rate: 0.00001\n",
      "Epoch 5472: train loss 0.0460\n",
      "Learning rate: 0.00001\n",
      "Epoch 5474: train loss 0.0460\n",
      "Learning rate: 0.00001\n",
      "Epoch 5476: train loss 0.0460\n",
      "Learning rate: 0.00001\n",
      "Epoch 5478: train loss 0.0460\n",
      "Learning rate: 0.00001\n",
      "Epoch 5480: train loss 0.0460\n",
      "Learning rate: 0.00001\n",
      "Epoch 5482: train loss 0.0460\n",
      "Learning rate: 0.00001\n",
      "Epoch 5484: train loss 0.0460\n",
      "Learning rate: 0.00001\n",
      "Epoch 5486: train loss 0.0460\n",
      "Learning rate: 0.00001\n",
      "Epoch 5488: train loss 0.0460\n",
      "Learning rate: 0.00001\n",
      "Epoch 5490: train loss 0.0460\n",
      "Learning rate: 0.00001\n",
      "Epoch 5492: train loss 0.0460\n",
      "Learning rate: 0.00001\n",
      "Epoch 5494: train loss 0.0460\n",
      "Learning rate: 0.00001\n",
      "Epoch 5496: train loss 0.0460\n",
      "Learning rate: 0.00001\n",
      "Epoch 5498: train loss 0.0460\n",
      "Learning rate: 0.00001\n",
      "Epoch 5500: train loss 0.0460\n",
      "Learning rate: 0.00001\n",
      "Epoch 5502: train loss 0.0460\n",
      "Learning rate: 0.00001\n",
      "Epoch 5504: train loss 0.0458\n",
      "Learning rate: 0.00001\n",
      "Epoch 5506: train loss 0.0458\n",
      "Learning rate: 0.00001\n",
      "Epoch 5508: train loss 0.0458\n",
      "Learning rate: 0.00001\n",
      "Epoch 5510: train loss 0.0458\n",
      "Learning rate: 0.00001\n",
      "Epoch 5512: train loss 0.0458\n",
      "Learning rate: 0.00001\n",
      "Epoch 5514: train loss 0.0458\n",
      "Learning rate: 0.00001\n",
      "Epoch 5516: train loss 0.0458\n",
      "Learning rate: 0.00001\n",
      "Epoch 5518: train loss 0.0458\n",
      "Learning rate: 0.00001\n",
      "Epoch 5520: train loss 0.0458\n",
      "Learning rate: 0.00001\n",
      "Epoch 5522: train loss 0.0458\n",
      "Learning rate: 0.00001\n",
      "Epoch 5524: train loss 0.0458\n",
      "Learning rate: 0.00001\n",
      "Epoch 5526: train loss 0.0458\n",
      "Learning rate: 0.00001\n",
      "Epoch 5528: train loss 0.0458\n",
      "Learning rate: 0.00001\n",
      "Epoch 5530: train loss 0.0458\n",
      "Learning rate: 0.00001\n",
      "Epoch 5532: train loss 0.0458\n",
      "Learning rate: 0.00001\n",
      "Epoch 5534: train loss 0.0458\n",
      "Learning rate: 0.00001\n",
      "Epoch 5536: train loss 0.0455\n",
      "Learning rate: 0.00001\n",
      "Epoch 5538: train loss 0.0455\n",
      "Learning rate: 0.00001\n",
      "Epoch 5540: train loss 0.0455\n",
      "Learning rate: 0.00001\n",
      "Epoch 5542: train loss 0.0455\n",
      "Learning rate: 0.00001\n",
      "Epoch 5544: train loss 0.0455\n",
      "Learning rate: 0.00001\n",
      "Epoch 5546: train loss 0.0455\n",
      "Learning rate: 0.00001\n",
      "Epoch 5548: train loss 0.0455\n",
      "Learning rate: 0.00001\n",
      "Epoch 5550: train loss 0.0455\n",
      "Learning rate: 0.00001\n",
      "Epoch 5552: train loss 0.0455\n",
      "Learning rate: 0.00001\n",
      "Epoch 5554: train loss 0.0455\n",
      "Learning rate: 0.00001\n",
      "Epoch 5556: train loss 0.0455\n",
      "Learning rate: 0.00001\n",
      "Epoch 5558: train loss 0.0455\n",
      "Learning rate: 0.00001\n",
      "Epoch 5560: train loss 0.0455\n",
      "Learning rate: 0.00001\n",
      "Epoch 5562: train loss 0.0455\n",
      "Learning rate: 0.00001\n",
      "Epoch 5564: train loss 0.0455\n",
      "Learning rate: 0.00001\n",
      "Epoch 5566: train loss 0.0455\n",
      "Learning rate: 0.00001\n",
      "Epoch 5568: train loss 0.0453\n",
      "Learning rate: 0.00001\n",
      "Epoch 5570: train loss 0.0453\n",
      "Learning rate: 0.00001\n",
      "Epoch 5572: train loss 0.0453\n",
      "Learning rate: 0.00001\n",
      "Epoch 5574: train loss 0.0453\n",
      "Learning rate: 0.00001\n",
      "Epoch 5576: train loss 0.0453\n",
      "Learning rate: 0.00001\n",
      "Epoch 5578: train loss 0.0453\n",
      "Learning rate: 0.00001\n",
      "Epoch 5580: train loss 0.0453\n",
      "Learning rate: 0.00001\n",
      "Epoch 5582: train loss 0.0453\n",
      "Learning rate: 0.00001\n",
      "Epoch 5584: train loss 0.0453\n",
      "Learning rate: 0.00001\n",
      "Epoch 5586: train loss 0.0453\n",
      "Learning rate: 0.00001\n",
      "Epoch 5588: train loss 0.0453\n",
      "Learning rate: 0.00001\n",
      "Epoch 5590: train loss 0.0453\n",
      "Learning rate: 0.00001\n",
      "Epoch 5592: train loss 0.0453\n",
      "Learning rate: 0.00001\n",
      "Epoch 5594: train loss 0.0453\n",
      "Learning rate: 0.00001\n",
      "Epoch 5596: train loss 0.0453\n",
      "Learning rate: 0.00001\n",
      "Epoch 5598: train loss 0.0453\n",
      "Learning rate: 0.00001\n",
      "Epoch 5600: train loss 0.0451\n",
      "Learning rate: 0.00001\n",
      "Epoch 5602: train loss 0.0451\n",
      "Learning rate: 0.00001\n",
      "Epoch 5604: train loss 0.0451\n",
      "Learning rate: 0.00001\n",
      "Epoch 5606: train loss 0.0451\n",
      "Learning rate: 0.00001\n",
      "Epoch 5608: train loss 0.0451\n",
      "Learning rate: 0.00001\n",
      "Epoch 5610: train loss 0.0451\n",
      "Learning rate: 0.00001\n",
      "Epoch 5612: train loss 0.0451\n",
      "Learning rate: 0.00001\n",
      "Epoch 5614: train loss 0.0451\n",
      "Learning rate: 0.00001\n",
      "Epoch 5616: train loss 0.0451\n",
      "Learning rate: 0.00001\n",
      "Epoch 5618: train loss 0.0451\n",
      "Learning rate: 0.00001\n",
      "Epoch 5620: train loss 0.0451\n",
      "Learning rate: 0.00001\n",
      "Epoch 5622: train loss 0.0451\n",
      "Learning rate: 0.00001\n",
      "Epoch 5624: train loss 0.0451\n",
      "Learning rate: 0.00001\n",
      "Epoch 5626: train loss 0.0451\n",
      "Learning rate: 0.00001\n",
      "Epoch 5628: train loss 0.0451\n",
      "Learning rate: 0.00001\n",
      "Epoch 5630: train loss 0.0451\n",
      "Learning rate: 0.00001\n",
      "Epoch 5632: train loss 0.0448\n",
      "Learning rate: 0.00001\n",
      "Epoch 5634: train loss 0.0448\n",
      "Learning rate: 0.00001\n",
      "Epoch 5636: train loss 0.0448\n",
      "Learning rate: 0.00001\n",
      "Epoch 5638: train loss 0.0448\n",
      "Learning rate: 0.00001\n",
      "Epoch 5640: train loss 0.0448\n",
      "Learning rate: 0.00001\n",
      "Epoch 5642: train loss 0.0448\n",
      "Learning rate: 0.00001\n",
      "Epoch 5644: train loss 0.0448\n",
      "Learning rate: 0.00001\n",
      "Epoch 5646: train loss 0.0448\n",
      "Learning rate: 0.00001\n",
      "Epoch 5648: train loss 0.0448\n",
      "Learning rate: 0.00001\n",
      "Epoch 5650: train loss 0.0448\n",
      "Learning rate: 0.00001\n",
      "Epoch 5652: train loss 0.0448\n",
      "Learning rate: 0.00001\n",
      "Epoch 5654: train loss 0.0448\n",
      "Learning rate: 0.00001\n",
      "Epoch 5656: train loss 0.0448\n",
      "Learning rate: 0.00001\n",
      "Epoch 5658: train loss 0.0448\n",
      "Learning rate: 0.00001\n",
      "Epoch 5660: train loss 0.0448\n",
      "Learning rate: 0.00001\n",
      "Epoch 5662: train loss 0.0448\n",
      "Learning rate: 0.00001\n",
      "Epoch 5664: train loss 0.0446\n",
      "Learning rate: 0.00001\n",
      "Epoch 5666: train loss 0.0446\n",
      "Learning rate: 0.00001\n",
      "Epoch 5668: train loss 0.0446\n",
      "Learning rate: 0.00001\n",
      "Epoch 5670: train loss 0.0446\n",
      "Learning rate: 0.00001\n",
      "Epoch 5672: train loss 0.0446\n",
      "Learning rate: 0.00001\n",
      "Epoch 5674: train loss 0.0446\n",
      "Learning rate: 0.00001\n",
      "Epoch 5676: train loss 0.0446\n",
      "Learning rate: 0.00001\n",
      "Epoch 5678: train loss 0.0446\n",
      "Learning rate: 0.00001\n",
      "Epoch 5680: train loss 0.0446\n",
      "Learning rate: 0.00001\n",
      "Epoch 5682: train loss 0.0446\n",
      "Learning rate: 0.00001\n",
      "Epoch 5684: train loss 0.0446\n",
      "Learning rate: 0.00001\n",
      "Epoch 5686: train loss 0.0446\n",
      "Learning rate: 0.00001\n",
      "Epoch 5688: train loss 0.0446\n",
      "Learning rate: 0.00001\n",
      "Epoch 5690: train loss 0.0446\n",
      "Learning rate: 0.00001\n",
      "Epoch 5692: train loss 0.0446\n",
      "Learning rate: 0.00001\n",
      "Epoch 5694: train loss 0.0446\n",
      "Learning rate: 0.00001\n",
      "Epoch 5696: train loss 0.0443\n",
      "Learning rate: 0.00001\n",
      "Epoch 5698: train loss 0.0443\n",
      "Learning rate: 0.00001\n",
      "Epoch 5700: train loss 0.0443\n",
      "Learning rate: 0.00001\n",
      "Epoch 5702: train loss 0.0443\n",
      "Learning rate: 0.00001\n",
      "Epoch 5704: train loss 0.0443\n",
      "Learning rate: 0.00001\n",
      "Epoch 5706: train loss 0.0443\n",
      "Learning rate: 0.00001\n",
      "Epoch 5708: train loss 0.0443\n",
      "Learning rate: 0.00001\n",
      "Epoch 5710: train loss 0.0443\n",
      "Learning rate: 0.00001\n",
      "Epoch 5712: train loss 0.0443\n",
      "Learning rate: 0.00001\n",
      "Epoch 5714: train loss 0.0443\n",
      "Learning rate: 0.00001\n",
      "Epoch 5716: train loss 0.0443\n",
      "Learning rate: 0.00001\n",
      "Epoch 5718: train loss 0.0443\n",
      "Learning rate: 0.00001\n",
      "Epoch 5720: train loss 0.0443\n",
      "Learning rate: 0.00001\n",
      "Epoch 5722: train loss 0.0443\n",
      "Learning rate: 0.00001\n",
      "Epoch 5724: train loss 0.0443\n",
      "Learning rate: 0.00001\n",
      "Epoch 5726: train loss 0.0443\n",
      "Learning rate: 0.00001\n",
      "Epoch 5728: train loss 0.0441\n",
      "Learning rate: 0.00001\n",
      "Epoch 5730: train loss 0.0441\n",
      "Learning rate: 0.00001\n",
      "Epoch 5732: train loss 0.0441\n",
      "Learning rate: 0.00001\n",
      "Epoch 5734: train loss 0.0441\n",
      "Learning rate: 0.00001\n",
      "Epoch 5736: train loss 0.0441\n",
      "Learning rate: 0.00001\n",
      "Epoch 5738: train loss 0.0441\n",
      "Learning rate: 0.00001\n",
      "Epoch 5740: train loss 0.0441\n",
      "Learning rate: 0.00001\n",
      "Epoch 5742: train loss 0.0441\n",
      "Learning rate: 0.00001\n",
      "Epoch 5744: train loss 0.0441\n",
      "Learning rate: 0.00001\n",
      "Epoch 5746: train loss 0.0441\n",
      "Learning rate: 0.00001\n",
      "Epoch 5748: train loss 0.0441\n",
      "Learning rate: 0.00001\n",
      "Epoch 5750: train loss 0.0441\n",
      "Learning rate: 0.00001\n",
      "Epoch 5752: train loss 0.0441\n",
      "Learning rate: 0.00001\n",
      "Epoch 5754: train loss 0.0441\n",
      "Learning rate: 0.00001\n",
      "Epoch 5756: train loss 0.0441\n",
      "Learning rate: 0.00001\n",
      "Epoch 5758: train loss 0.0441\n",
      "Learning rate: 0.00001\n",
      "Epoch 5760: train loss 0.0439\n",
      "Learning rate: 0.00001\n",
      "Epoch 5762: train loss 0.0439\n",
      "Learning rate: 0.00001\n",
      "Epoch 5764: train loss 0.0439\n",
      "Learning rate: 0.00001\n",
      "Epoch 5766: train loss 0.0439\n",
      "Learning rate: 0.00001\n",
      "Epoch 5768: train loss 0.0439\n",
      "Learning rate: 0.00001\n",
      "Epoch 5770: train loss 0.0439\n",
      "Learning rate: 0.00001\n",
      "Epoch 5772: train loss 0.0439\n",
      "Learning rate: 0.00001\n",
      "Epoch 5774: train loss 0.0439\n",
      "Learning rate: 0.00001\n",
      "Epoch 5776: train loss 0.0439\n",
      "Learning rate: 0.00001\n",
      "Epoch 5778: train loss 0.0439\n",
      "Learning rate: 0.00001\n",
      "Epoch 5780: train loss 0.0439\n",
      "Learning rate: 0.00001\n",
      "Epoch 5782: train loss 0.0439\n",
      "Learning rate: 0.00001\n",
      "Epoch 5784: train loss 0.0439\n",
      "Learning rate: 0.00001\n",
      "Epoch 5786: train loss 0.0439\n",
      "Learning rate: 0.00001\n",
      "Epoch 5788: train loss 0.0439\n",
      "Learning rate: 0.00001\n",
      "Epoch 5790: train loss 0.0439\n",
      "Learning rate: 0.00001\n",
      "Epoch 5792: train loss 0.0437\n",
      "Learning rate: 0.00001\n",
      "Epoch 5794: train loss 0.0437\n",
      "Learning rate: 0.00001\n",
      "Epoch 5796: train loss 0.0437\n",
      "Learning rate: 0.00001\n",
      "Epoch 5798: train loss 0.0437\n",
      "Learning rate: 0.00001\n",
      "Epoch 5800: train loss 0.0437\n",
      "Learning rate: 0.00001\n",
      "Epoch 5802: train loss 0.0437\n",
      "Learning rate: 0.00001\n",
      "Epoch 5804: train loss 0.0437\n",
      "Learning rate: 0.00001\n",
      "Epoch 5806: train loss 0.0437\n",
      "Learning rate: 0.00001\n",
      "Epoch 5808: train loss 0.0437\n",
      "Learning rate: 0.00001\n",
      "Epoch 5810: train loss 0.0437\n",
      "Learning rate: 0.00001\n",
      "Epoch 5812: train loss 0.0437\n",
      "Learning rate: 0.00001\n",
      "Epoch 5814: train loss 0.0437\n",
      "Learning rate: 0.00001\n",
      "Epoch 5816: train loss 0.0437\n",
      "Learning rate: 0.00001\n",
      "Epoch 5818: train loss 0.0437\n",
      "Learning rate: 0.00001\n",
      "Epoch 5820: train loss 0.0437\n",
      "Learning rate: 0.00001\n",
      "Epoch 5822: train loss 0.0437\n",
      "Learning rate: 0.00001\n",
      "Epoch 5824: train loss 0.0426\n",
      "Learning rate: 0.00001\n",
      "Epoch 5826: train loss 0.0426\n",
      "Learning rate: 0.00001\n",
      "Epoch 5828: train loss 0.0426\n",
      "Learning rate: 0.00001\n",
      "Epoch 5830: train loss 0.0426\n",
      "Learning rate: 0.00001\n",
      "Epoch 5832: train loss 0.0426\n",
      "Learning rate: 0.00001\n",
      "Epoch 5834: train loss 0.0426\n",
      "Learning rate: 0.00001\n",
      "Epoch 5836: train loss 0.0426\n",
      "Learning rate: 0.00001\n",
      "Epoch 5838: train loss 0.0426\n",
      "Learning rate: 0.00001\n",
      "Epoch 5840: train loss 0.0426\n",
      "Learning rate: 0.00001\n",
      "Epoch 5842: train loss 0.0426\n",
      "Learning rate: 0.00001\n",
      "Epoch 5844: train loss 0.0426\n",
      "Learning rate: 0.00001\n",
      "Epoch 5846: train loss 0.0426\n",
      "Learning rate: 0.00001\n",
      "Epoch 5848: train loss 0.0426\n",
      "Learning rate: 0.00001\n",
      "Epoch 5850: train loss 0.0426\n",
      "Learning rate: 0.00001\n",
      "Epoch 5852: train loss 0.0426\n",
      "Learning rate: 0.00001\n",
      "Epoch 5854: train loss 0.0426\n",
      "Learning rate: 0.00001\n",
      "Epoch 5856: train loss 0.0410\n",
      "Learning rate: 0.00001\n",
      "Epoch 5858: train loss 0.0410\n",
      "Learning rate: 0.00001\n",
      "Epoch 5860: train loss 0.0410\n",
      "Learning rate: 0.00001\n",
      "Epoch 5862: train loss 0.0410\n",
      "Learning rate: 0.00001\n",
      "Epoch 5864: train loss 0.0410\n",
      "Learning rate: 0.00001\n",
      "Epoch 5866: train loss 0.0410\n",
      "Learning rate: 0.00001\n",
      "Epoch 5868: train loss 0.0410\n",
      "Learning rate: 0.00001\n",
      "Epoch 5870: train loss 0.0410\n",
      "Learning rate: 0.00001\n",
      "Epoch 5872: train loss 0.0410\n",
      "Learning rate: 0.00001\n",
      "Epoch 5874: train loss 0.0410\n",
      "Learning rate: 0.00001\n",
      "Epoch 5876: train loss 0.0410\n",
      "Learning rate: 0.00001\n",
      "Epoch 5878: train loss 0.0410\n",
      "Learning rate: 0.00001\n",
      "Epoch 5880: train loss 0.0410\n",
      "Learning rate: 0.00001\n",
      "Epoch 5882: train loss 0.0410\n",
      "Learning rate: 0.00001\n",
      "Epoch 5884: train loss 0.0410\n",
      "Learning rate: 0.00001\n",
      "Epoch 5886: train loss 0.0410\n",
      "Learning rate: 0.00001\n",
      "Epoch 5888: train loss 0.0405\n",
      "Learning rate: 0.00001\n",
      "Epoch 5890: train loss 0.0405\n",
      "Learning rate: 0.00001\n",
      "Epoch 5892: train loss 0.0405\n",
      "Learning rate: 0.00001\n",
      "Epoch 5894: train loss 0.0405\n",
      "Learning rate: 0.00001\n",
      "Epoch 5896: train loss 0.0405\n",
      "Learning rate: 0.00001\n",
      "Epoch 5898: train loss 0.0405\n",
      "Learning rate: 0.00001\n",
      "Epoch 5900: train loss 0.0405\n",
      "Learning rate: 0.00001\n",
      "Epoch 5902: train loss 0.0405\n",
      "Learning rate: 0.00001\n",
      "Epoch 5904: train loss 0.0405\n",
      "Learning rate: 0.00001\n",
      "Epoch 5906: train loss 0.0405\n",
      "Learning rate: 0.00001\n",
      "Epoch 5908: train loss 0.0405\n",
      "Learning rate: 0.00001\n",
      "Epoch 5910: train loss 0.0405\n",
      "Learning rate: 0.00001\n",
      "Epoch 5912: train loss 0.0405\n",
      "Learning rate: 0.00001\n",
      "Epoch 5914: train loss 0.0405\n",
      "Learning rate: 0.00001\n",
      "Epoch 5916: train loss 0.0405\n",
      "Learning rate: 0.00001\n",
      "Epoch 5918: train loss 0.0405\n",
      "Learning rate: 0.00001\n",
      "Epoch 5920: train loss 0.0403\n",
      "Learning rate: 0.00001\n",
      "Epoch 5922: train loss 0.0403\n",
      "Learning rate: 0.00001\n",
      "Epoch 5924: train loss 0.0403\n",
      "Learning rate: 0.00001\n",
      "Epoch 5926: train loss 0.0403\n",
      "Learning rate: 0.00001\n",
      "Epoch 5928: train loss 0.0403\n",
      "Learning rate: 0.00001\n",
      "Epoch 5930: train loss 0.0403\n",
      "Learning rate: 0.00001\n",
      "Epoch 5932: train loss 0.0403\n",
      "Learning rate: 0.00001\n",
      "Epoch 5934: train loss 0.0403\n",
      "Learning rate: 0.00001\n",
      "Epoch 5936: train loss 0.0403\n",
      "Learning rate: 0.00001\n",
      "Epoch 5938: train loss 0.0403\n",
      "Learning rate: 0.00001\n",
      "Epoch 5940: train loss 0.0403\n",
      "Learning rate: 0.00001\n",
      "Epoch 5942: train loss 0.0403\n",
      "Learning rate: 0.00001\n",
      "Epoch 5944: train loss 0.0403\n",
      "Learning rate: 0.00001\n",
      "Epoch 5946: train loss 0.0403\n",
      "Learning rate: 0.00001\n",
      "Epoch 5948: train loss 0.0403\n",
      "Learning rate: 0.00001\n",
      "Epoch 5950: train loss 0.0403\n",
      "Learning rate: 0.00001\n",
      "Epoch 5952: train loss 0.0401\n",
      "Learning rate: 0.00001\n",
      "Epoch 5954: train loss 0.0401\n",
      "Learning rate: 0.00001\n",
      "Epoch 5956: train loss 0.0401\n",
      "Learning rate: 0.00001\n",
      "Epoch 5958: train loss 0.0401\n",
      "Learning rate: 0.00001\n",
      "Epoch 5960: train loss 0.0401\n",
      "Learning rate: 0.00001\n",
      "Epoch 5962: train loss 0.0401\n",
      "Learning rate: 0.00001\n",
      "Epoch 5964: train loss 0.0401\n",
      "Learning rate: 0.00001\n",
      "Epoch 5966: train loss 0.0401\n",
      "Learning rate: 0.00001\n",
      "Epoch 5968: train loss 0.0401\n",
      "Learning rate: 0.00001\n",
      "Epoch 5970: train loss 0.0401\n",
      "Learning rate: 0.00001\n",
      "Epoch 5972: train loss 0.0401\n",
      "Learning rate: 0.00001\n",
      "Epoch 5974: train loss 0.0401\n",
      "Learning rate: 0.00001\n",
      "Epoch 5976: train loss 0.0401\n",
      "Learning rate: 0.00001\n",
      "Epoch 5978: train loss 0.0401\n",
      "Learning rate: 0.00001\n",
      "Epoch 5980: train loss 0.0401\n",
      "Learning rate: 0.00001\n",
      "Epoch 5982: train loss 0.0401\n",
      "Learning rate: 0.00001\n",
      "Epoch 5984: train loss 0.0400\n",
      "Learning rate: 0.00001\n",
      "Epoch 5986: train loss 0.0400\n",
      "Learning rate: 0.00001\n",
      "Epoch 5988: train loss 0.0400\n",
      "Learning rate: 0.00001\n",
      "Epoch 5990: train loss 0.0400\n",
      "Learning rate: 0.00001\n",
      "Epoch 5992: train loss 0.0400\n",
      "Learning rate: 0.00001\n",
      "Epoch 5994: train loss 0.0400\n",
      "Learning rate: 0.00001\n",
      "Epoch 5996: train loss 0.0400\n",
      "Learning rate: 0.00001\n",
      "Epoch 5998: train loss 0.0400\n",
      "Learning rate: 0.00001\n",
      "Epoch 6000: train loss 0.0400\n",
      "Learning rate: 0.00001\n",
      "Epoch 6002: train loss 0.0400\n",
      "Learning rate: 0.00001\n",
      "Epoch 6004: train loss 0.0400\n",
      "Learning rate: 0.00001\n",
      "Epoch 6006: train loss 0.0400\n",
      "Learning rate: 0.00001\n",
      "Epoch 6008: train loss 0.0400\n",
      "Learning rate: 0.00001\n",
      "Epoch 6010: train loss 0.0400\n",
      "Learning rate: 0.00001\n",
      "Epoch 6012: train loss 0.0400\n",
      "Learning rate: 0.00001\n",
      "Epoch 6014: train loss 0.0400\n",
      "Learning rate: 0.00001\n",
      "Epoch 6016: train loss 0.0398\n",
      "Learning rate: 0.00001\n",
      "Epoch 6018: train loss 0.0398\n",
      "Learning rate: 0.00001\n",
      "Epoch 6020: train loss 0.0398\n",
      "Learning rate: 0.00001\n",
      "Epoch 6022: train loss 0.0398\n",
      "Learning rate: 0.00001\n",
      "Epoch 6024: train loss 0.0398\n",
      "Learning rate: 0.00001\n",
      "Epoch 6026: train loss 0.0398\n",
      "Learning rate: 0.00001\n",
      "Epoch 6028: train loss 0.0398\n",
      "Learning rate: 0.00001\n",
      "Epoch 6030: train loss 0.0398\n",
      "Learning rate: 0.00001\n",
      "Epoch 6032: train loss 0.0398\n",
      "Learning rate: 0.00001\n",
      "Epoch 6034: train loss 0.0398\n",
      "Learning rate: 0.00001\n",
      "Epoch 6036: train loss 0.0398\n",
      "Learning rate: 0.00001\n",
      "Epoch 6038: train loss 0.0398\n",
      "Learning rate: 0.00001\n",
      "Epoch 6040: train loss 0.0398\n",
      "Learning rate: 0.00001\n",
      "Epoch 6042: train loss 0.0398\n",
      "Learning rate: 0.00001\n",
      "Epoch 6044: train loss 0.0398\n",
      "Learning rate: 0.00001\n",
      "Epoch 6046: train loss 0.0398\n",
      "Learning rate: 0.00001\n",
      "Epoch 6048: train loss 0.0396\n",
      "Learning rate: 0.00001\n",
      "Epoch 6050: train loss 0.0396\n",
      "Learning rate: 0.00001\n",
      "Epoch 6052: train loss 0.0396\n",
      "Learning rate: 0.00001\n",
      "Epoch 6054: train loss 0.0396\n",
      "Learning rate: 0.00001\n",
      "Epoch 6056: train loss 0.0396\n",
      "Learning rate: 0.00001\n",
      "Epoch 6058: train loss 0.0396\n",
      "Learning rate: 0.00001\n",
      "Epoch 6060: train loss 0.0396\n",
      "Learning rate: 0.00001\n",
      "Epoch 6062: train loss 0.0396\n",
      "Learning rate: 0.00001\n",
      "Epoch 6064: train loss 0.0396\n",
      "Learning rate: 0.00001\n",
      "Epoch 6066: train loss 0.0396\n",
      "Learning rate: 0.00001\n",
      "Epoch 6068: train loss 0.0396\n",
      "Learning rate: 0.00001\n",
      "Epoch 6070: train loss 0.0396\n",
      "Learning rate: 0.00001\n",
      "Epoch 6072: train loss 0.0396\n",
      "Learning rate: 0.00001\n",
      "Epoch 6074: train loss 0.0396\n",
      "Learning rate: 0.00001\n",
      "Epoch 6076: train loss 0.0396\n",
      "Learning rate: 0.00001\n",
      "Epoch 6078: train loss 0.0396\n",
      "Learning rate: 0.00001\n",
      "Epoch 6080: train loss 0.0395\n",
      "Learning rate: 0.00001\n",
      "Epoch 6082: train loss 0.0395\n",
      "Learning rate: 0.00001\n",
      "Epoch 6084: train loss 0.0395\n",
      "Learning rate: 0.00001\n",
      "Epoch 6086: train loss 0.0395\n",
      "Learning rate: 0.00001\n",
      "Epoch 6088: train loss 0.0395\n",
      "Learning rate: 0.00001\n",
      "Epoch 6090: train loss 0.0395\n",
      "Learning rate: 0.00001\n",
      "Epoch 6092: train loss 0.0395\n",
      "Learning rate: 0.00001\n",
      "Epoch 6094: train loss 0.0395\n",
      "Learning rate: 0.00001\n",
      "Epoch 6096: train loss 0.0395\n",
      "Learning rate: 0.00001\n",
      "Epoch 6098: train loss 0.0395\n",
      "Learning rate: 0.00001\n",
      "Epoch 6100: train loss 0.0395\n",
      "Learning rate: 0.00001\n",
      "Epoch 6102: train loss 0.0395\n",
      "Learning rate: 0.00001\n",
      "Epoch 6104: train loss 0.0395\n",
      "Learning rate: 0.00001\n",
      "Epoch 6106: train loss 0.0395\n",
      "Learning rate: 0.00001\n",
      "Epoch 6108: train loss 0.0395\n",
      "Learning rate: 0.00001\n",
      "Epoch 6110: train loss 0.0395\n",
      "Learning rate: 0.00001\n",
      "Epoch 6112: train loss 0.0393\n",
      "Learning rate: 0.00001\n",
      "Epoch 6114: train loss 0.0393\n",
      "Learning rate: 0.00001\n",
      "Epoch 6116: train loss 0.0393\n",
      "Learning rate: 0.00001\n",
      "Epoch 6118: train loss 0.0393\n",
      "Learning rate: 0.00001\n",
      "Epoch 6120: train loss 0.0393\n",
      "Learning rate: 0.00001\n",
      "Epoch 6122: train loss 0.0393\n",
      "Learning rate: 0.00001\n",
      "Epoch 6124: train loss 0.0393\n",
      "Learning rate: 0.00001\n",
      "Epoch 6126: train loss 0.0393\n",
      "Learning rate: 0.00001\n",
      "Epoch 6128: train loss 0.0393\n",
      "Learning rate: 0.00001\n",
      "Epoch 6130: train loss 0.0393\n",
      "Learning rate: 0.00001\n",
      "Epoch 6132: train loss 0.0393\n",
      "Learning rate: 0.00001\n",
      "Epoch 6134: train loss 0.0393\n",
      "Learning rate: 0.00001\n",
      "Epoch 6136: train loss 0.0393\n",
      "Learning rate: 0.00001\n",
      "Epoch 6138: train loss 0.0393\n",
      "Learning rate: 0.00001\n",
      "Epoch 6140: train loss 0.0393\n",
      "Learning rate: 0.00001\n",
      "Epoch 6142: train loss 0.0393\n",
      "Learning rate: 0.00001\n",
      "Epoch 6144: train loss 0.0391\n",
      "Learning rate: 0.00001\n",
      "Epoch 6146: train loss 0.0391\n",
      "Learning rate: 0.00001\n",
      "Epoch 6148: train loss 0.0391\n",
      "Learning rate: 0.00001\n",
      "Epoch 6150: train loss 0.0391\n",
      "Learning rate: 0.00001\n",
      "Epoch 6152: train loss 0.0391\n",
      "Learning rate: 0.00001\n",
      "Epoch 6154: train loss 0.0391\n",
      "Learning rate: 0.00001\n",
      "Epoch 6156: train loss 0.0391\n",
      "Learning rate: 0.00001\n",
      "Epoch 6158: train loss 0.0391\n",
      "Learning rate: 0.00001\n",
      "Epoch 6160: train loss 0.0391\n",
      "Learning rate: 0.00001\n",
      "Epoch 6162: train loss 0.0391\n",
      "Learning rate: 0.00001\n",
      "Epoch 6164: train loss 0.0391\n",
      "Learning rate: 0.00001\n",
      "Epoch 6166: train loss 0.0391\n",
      "Learning rate: 0.00001\n",
      "Epoch 6168: train loss 0.0391\n",
      "Learning rate: 0.00001\n",
      "Epoch 6170: train loss 0.0391\n",
      "Learning rate: 0.00001\n",
      "Epoch 6172: train loss 0.0391\n",
      "Learning rate: 0.00001\n",
      "Epoch 6174: train loss 0.0391\n",
      "Learning rate: 0.00001\n",
      "Epoch 6176: train loss 0.0390\n",
      "Learning rate: 0.00001\n",
      "Epoch 6178: train loss 0.0390\n",
      "Learning rate: 0.00001\n",
      "Epoch 6180: train loss 0.0390\n",
      "Learning rate: 0.00001\n",
      "Epoch 6182: train loss 0.0390\n",
      "Learning rate: 0.00001\n",
      "Epoch 6184: train loss 0.0390\n",
      "Learning rate: 0.00001\n",
      "Epoch 6186: train loss 0.0390\n",
      "Learning rate: 0.00001\n",
      "Epoch 6188: train loss 0.0390\n",
      "Learning rate: 0.00001\n",
      "Epoch 6190: train loss 0.0390\n",
      "Learning rate: 0.00001\n",
      "Epoch 6192: train loss 0.0390\n",
      "Learning rate: 0.00001\n",
      "Epoch 6194: train loss 0.0390\n",
      "Learning rate: 0.00001\n",
      "Epoch 6196: train loss 0.0390\n",
      "Learning rate: 0.00001\n",
      "Epoch 6198: train loss 0.0390\n",
      "Learning rate: 0.00001\n",
      "Epoch 6200: train loss 0.0390\n",
      "Learning rate: 0.00001\n",
      "Epoch 6202: train loss 0.0390\n",
      "Learning rate: 0.00001\n",
      "Epoch 6204: train loss 0.0390\n",
      "Learning rate: 0.00001\n",
      "Epoch 6206: train loss 0.0390\n",
      "Learning rate: 0.00001\n",
      "Epoch 6208: train loss 0.0388\n",
      "Learning rate: 0.00001\n",
      "Epoch 6210: train loss 0.0388\n",
      "Learning rate: 0.00001\n",
      "Epoch 6212: train loss 0.0388\n",
      "Learning rate: 0.00001\n",
      "Epoch 6214: train loss 0.0388\n",
      "Learning rate: 0.00001\n",
      "Epoch 6216: train loss 0.0388\n",
      "Learning rate: 0.00001\n",
      "Epoch 6218: train loss 0.0388\n",
      "Learning rate: 0.00001\n",
      "Epoch 6220: train loss 0.0388\n",
      "Learning rate: 0.00001\n",
      "Epoch 6222: train loss 0.0388\n",
      "Learning rate: 0.00001\n",
      "Epoch 6224: train loss 0.0388\n",
      "Learning rate: 0.00001\n",
      "Epoch 6226: train loss 0.0388\n",
      "Learning rate: 0.00001\n",
      "Epoch 6228: train loss 0.0388\n",
      "Learning rate: 0.00001\n",
      "Epoch 6230: train loss 0.0388\n",
      "Learning rate: 0.00001\n",
      "Epoch 6232: train loss 0.0388\n",
      "Learning rate: 0.00001\n",
      "Epoch 6234: train loss 0.0388\n",
      "Learning rate: 0.00001\n",
      "Epoch 6236: train loss 0.0388\n",
      "Learning rate: 0.00001\n",
      "Epoch 6238: train loss 0.0388\n",
      "Learning rate: 0.00001\n",
      "Epoch 6240: train loss 0.0386\n",
      "Learning rate: 0.00001\n",
      "Epoch 6242: train loss 0.0386\n",
      "Learning rate: 0.00001\n",
      "Epoch 6244: train loss 0.0386\n",
      "Learning rate: 0.00001\n",
      "Epoch 6246: train loss 0.0386\n",
      "Learning rate: 0.00001\n",
      "Epoch 6248: train loss 0.0386\n",
      "Learning rate: 0.00001\n",
      "Epoch 6250: train loss 0.0386\n",
      "Learning rate: 0.00001\n",
      "Epoch 6252: train loss 0.0386\n",
      "Learning rate: 0.00001\n",
      "Epoch 6254: train loss 0.0386\n",
      "Learning rate: 0.00001\n",
      "Epoch 6256: train loss 0.0386\n",
      "Learning rate: 0.00001\n",
      "Epoch 6258: train loss 0.0386\n",
      "Learning rate: 0.00001\n",
      "Epoch 6260: train loss 0.0386\n",
      "Learning rate: 0.00001\n",
      "Epoch 6262: train loss 0.0386\n",
      "Learning rate: 0.00001\n",
      "Epoch 6264: train loss 0.0386\n",
      "Learning rate: 0.00001\n",
      "Epoch 6266: train loss 0.0386\n",
      "Learning rate: 0.00001\n",
      "Epoch 6268: train loss 0.0386\n",
      "Learning rate: 0.00001\n",
      "Epoch 6270: train loss 0.0386\n",
      "Learning rate: 0.00001\n",
      "Epoch 6272: train loss 0.0385\n",
      "Learning rate: 0.00001\n",
      "Epoch 6274: train loss 0.0385\n",
      "Learning rate: 0.00001\n",
      "Epoch 6276: train loss 0.0385\n",
      "Learning rate: 0.00001\n",
      "Epoch 6278: train loss 0.0385\n",
      "Learning rate: 0.00001\n",
      "Epoch 6280: train loss 0.0385\n",
      "Learning rate: 0.00001\n",
      "Epoch 6282: train loss 0.0385\n",
      "Learning rate: 0.00001\n",
      "Epoch 6284: train loss 0.0385\n",
      "Learning rate: 0.00001\n",
      "Epoch 6286: train loss 0.0385\n",
      "Learning rate: 0.00001\n",
      "Epoch 6288: train loss 0.0385\n",
      "Learning rate: 0.00001\n",
      "Epoch 6290: train loss 0.0385\n",
      "Learning rate: 0.00001\n",
      "Epoch 6292: train loss 0.0385\n",
      "Learning rate: 0.00001\n",
      "Epoch 6294: train loss 0.0385\n",
      "Learning rate: 0.00001\n",
      "Epoch 6296: train loss 0.0385\n",
      "Learning rate: 0.00001\n",
      "Epoch 6298: train loss 0.0385\n",
      "Learning rate: 0.00001\n",
      "Epoch 6300: train loss 0.0385\n",
      "Learning rate: 0.00001\n",
      "Epoch 6302: train loss 0.0385\n",
      "Learning rate: 0.00001\n",
      "Epoch 6304: train loss 0.0382\n",
      "Learning rate: 0.00001\n",
      "Epoch 6306: train loss 0.0382\n",
      "Learning rate: 0.00001\n",
      "Epoch 6308: train loss 0.0382\n",
      "Learning rate: 0.00001\n",
      "Epoch 6310: train loss 0.0382\n",
      "Learning rate: 0.00001\n",
      "Epoch 6312: train loss 0.0382\n",
      "Learning rate: 0.00001\n",
      "Epoch 6314: train loss 0.0382\n",
      "Learning rate: 0.00001\n",
      "Epoch 6316: train loss 0.0382\n",
      "Learning rate: 0.00001\n",
      "Epoch 6318: train loss 0.0382\n",
      "Learning rate: 0.00001\n",
      "Epoch 6320: train loss 0.0382\n",
      "Learning rate: 0.00001\n",
      "Epoch 6322: train loss 0.0382\n",
      "Learning rate: 0.00001\n",
      "Epoch 6324: train loss 0.0382\n",
      "Learning rate: 0.00001\n",
      "Epoch 6326: train loss 0.0382\n",
      "Learning rate: 0.00001\n",
      "Epoch 6328: train loss 0.0382\n",
      "Learning rate: 0.00001\n",
      "Epoch 6330: train loss 0.0382\n",
      "Learning rate: 0.00001\n",
      "Epoch 6332: train loss 0.0382\n",
      "Learning rate: 0.00001\n",
      "Epoch 6334: train loss 0.0382\n",
      "Learning rate: 0.00001\n",
      "Epoch 6336: train loss 0.0367\n",
      "Learning rate: 0.00001\n",
      "Epoch 6338: train loss 0.0367\n",
      "Learning rate: 0.00001\n",
      "Epoch 6340: train loss 0.0367\n",
      "Learning rate: 0.00001\n",
      "Epoch 6342: train loss 0.0367\n",
      "Learning rate: 0.00001\n",
      "Epoch 6344: train loss 0.0367\n",
      "Learning rate: 0.00001\n",
      "Epoch 6346: train loss 0.0367\n",
      "Learning rate: 0.00001\n",
      "Epoch 6348: train loss 0.0367\n",
      "Learning rate: 0.00001\n",
      "Epoch 6350: train loss 0.0367\n",
      "Learning rate: 0.00001\n",
      "Epoch 6352: train loss 0.0367\n",
      "Learning rate: 0.00001\n",
      "Epoch 6354: train loss 0.0367\n",
      "Learning rate: 0.00001\n",
      "Epoch 6356: train loss 0.0367\n",
      "Learning rate: 0.00001\n",
      "Epoch 6358: train loss 0.0367\n",
      "Learning rate: 0.00001\n",
      "Epoch 6360: train loss 0.0367\n",
      "Learning rate: 0.00001\n",
      "Epoch 6362: train loss 0.0367\n",
      "Learning rate: 0.00001\n",
      "Epoch 6364: train loss 0.0367\n",
      "Learning rate: 0.00001\n",
      "Epoch 6366: train loss 0.0367\n",
      "Learning rate: 0.00001\n",
      "Epoch 6368: train loss 0.0359\n",
      "Learning rate: 0.00001\n",
      "Epoch 6370: train loss 0.0359\n",
      "Learning rate: 0.00001\n",
      "Epoch 6372: train loss 0.0359\n",
      "Learning rate: 0.00001\n",
      "Epoch 6374: train loss 0.0359\n",
      "Learning rate: 0.00001\n",
      "Epoch 6376: train loss 0.0359\n",
      "Learning rate: 0.00001\n",
      "Epoch 6378: train loss 0.0359\n",
      "Learning rate: 0.00001\n",
      "Epoch 6380: train loss 0.0359\n",
      "Learning rate: 0.00001\n",
      "Epoch 6382: train loss 0.0359\n",
      "Learning rate: 0.00001\n",
      "Epoch 6384: train loss 0.0359\n",
      "Learning rate: 0.00001\n",
      "Epoch 6386: train loss 0.0359\n",
      "Learning rate: 0.00001\n",
      "Epoch 6388: train loss 0.0359\n",
      "Learning rate: 0.00001\n",
      "Epoch 6390: train loss 0.0359\n",
      "Learning rate: 0.00001\n",
      "Epoch 6392: train loss 0.0359\n",
      "Learning rate: 0.00001\n",
      "Epoch 6394: train loss 0.0359\n",
      "Learning rate: 0.00001\n",
      "Epoch 6396: train loss 0.0359\n",
      "Learning rate: 0.00001\n",
      "Epoch 6398: train loss 0.0359\n",
      "Learning rate: 0.00001\n",
      "Epoch 6400: train loss 0.0357\n",
      "Learning rate: 0.00001\n",
      "Epoch 6402: train loss 0.0357\n",
      "Learning rate: 0.00001\n",
      "Epoch 6404: train loss 0.0357\n",
      "Learning rate: 0.00001\n",
      "Epoch 6406: train loss 0.0357\n",
      "Learning rate: 0.00001\n",
      "Epoch 6408: train loss 0.0357\n",
      "Learning rate: 0.00001\n",
      "Epoch 6410: train loss 0.0357\n",
      "Learning rate: 0.00001\n",
      "Epoch 6412: train loss 0.0357\n",
      "Learning rate: 0.00001\n",
      "Epoch 6414: train loss 0.0357\n",
      "Learning rate: 0.00001\n",
      "Epoch 6416: train loss 0.0357\n",
      "Learning rate: 0.00001\n",
      "Epoch 6418: train loss 0.0357\n",
      "Learning rate: 0.00001\n",
      "Epoch 6420: train loss 0.0357\n",
      "Learning rate: 0.00001\n",
      "Epoch 6422: train loss 0.0357\n",
      "Learning rate: 0.00001\n",
      "Epoch 6424: train loss 0.0357\n",
      "Learning rate: 0.00001\n",
      "Epoch 6426: train loss 0.0357\n",
      "Learning rate: 0.00001\n",
      "Epoch 6428: train loss 0.0357\n",
      "Learning rate: 0.00001\n",
      "Epoch 6430: train loss 0.0357\n",
      "Learning rate: 0.00001\n",
      "Epoch 6432: train loss 0.0355\n",
      "Learning rate: 0.00001\n",
      "Epoch 6434: train loss 0.0355\n",
      "Learning rate: 0.00001\n",
      "Epoch 6436: train loss 0.0355\n",
      "Learning rate: 0.00001\n",
      "Epoch 6438: train loss 0.0355\n",
      "Learning rate: 0.00001\n",
      "Epoch 6440: train loss 0.0355\n",
      "Learning rate: 0.00001\n",
      "Epoch 6442: train loss 0.0355\n",
      "Learning rate: 0.00001\n",
      "Epoch 6444: train loss 0.0355\n",
      "Learning rate: 0.00001\n",
      "Epoch 6446: train loss 0.0355\n",
      "Learning rate: 0.00001\n",
      "Epoch 6448: train loss 0.0355\n",
      "Learning rate: 0.00001\n",
      "Epoch 6450: train loss 0.0355\n",
      "Learning rate: 0.00001\n",
      "Epoch 6452: train loss 0.0355\n",
      "Learning rate: 0.00001\n",
      "Epoch 6454: train loss 0.0355\n",
      "Learning rate: 0.00001\n",
      "Epoch 6456: train loss 0.0355\n",
      "Learning rate: 0.00001\n",
      "Epoch 6458: train loss 0.0355\n",
      "Learning rate: 0.00001\n",
      "Epoch 6460: train loss 0.0355\n",
      "Learning rate: 0.00001\n",
      "Epoch 6462: train loss 0.0355\n",
      "Learning rate: 0.00001\n",
      "Epoch 6464: train loss 0.0354\n",
      "Learning rate: 0.00001\n",
      "Epoch 6466: train loss 0.0354\n",
      "Learning rate: 0.00001\n",
      "Epoch 6468: train loss 0.0354\n",
      "Learning rate: 0.00001\n",
      "Epoch 6470: train loss 0.0354\n",
      "Learning rate: 0.00001\n",
      "Epoch 6472: train loss 0.0354\n",
      "Learning rate: 0.00001\n",
      "Epoch 6474: train loss 0.0354\n",
      "Learning rate: 0.00001\n",
      "Epoch 6476: train loss 0.0354\n",
      "Learning rate: 0.00001\n",
      "Epoch 6478: train loss 0.0354\n",
      "Learning rate: 0.00001\n",
      "Epoch 6480: train loss 0.0354\n",
      "Learning rate: 0.00001\n",
      "Epoch 6482: train loss 0.0354\n",
      "Learning rate: 0.00001\n",
      "Epoch 6484: train loss 0.0354\n",
      "Learning rate: 0.00001\n",
      "Epoch 6486: train loss 0.0354\n",
      "Learning rate: 0.00001\n",
      "Epoch 6488: train loss 0.0354\n",
      "Learning rate: 0.00001\n",
      "Epoch 6490: train loss 0.0354\n",
      "Learning rate: 0.00001\n",
      "Epoch 6492: train loss 0.0354\n",
      "Learning rate: 0.00001\n",
      "Epoch 6494: train loss 0.0354\n",
      "Learning rate: 0.00001\n",
      "Epoch 6496: train loss 0.0353\n",
      "Learning rate: 0.00001\n",
      "Epoch 6498: train loss 0.0353\n",
      "Learning rate: 0.00001\n",
      "Epoch 6500: train loss 0.0353\n",
      "Learning rate: 0.00001\n",
      "Epoch 6502: train loss 0.0353\n",
      "Learning rate: 0.00001\n",
      "Epoch 6504: train loss 0.0353\n",
      "Learning rate: 0.00001\n",
      "Epoch 6506: train loss 0.0353\n",
      "Learning rate: 0.00001\n",
      "Epoch 6508: train loss 0.0353\n",
      "Learning rate: 0.00001\n",
      "Epoch 6510: train loss 0.0353\n",
      "Learning rate: 0.00001\n",
      "Epoch 6512: train loss 0.0353\n",
      "Learning rate: 0.00001\n",
      "Epoch 6514: train loss 0.0353\n",
      "Learning rate: 0.00001\n",
      "Epoch 6516: train loss 0.0353\n",
      "Learning rate: 0.00001\n",
      "Epoch 6518: train loss 0.0353\n",
      "Learning rate: 0.00001\n",
      "Epoch 6520: train loss 0.0353\n",
      "Learning rate: 0.00001\n",
      "Epoch 6522: train loss 0.0353\n",
      "Learning rate: 0.00001\n",
      "Epoch 6524: train loss 0.0353\n",
      "Learning rate: 0.00001\n",
      "Epoch 6526: train loss 0.0353\n",
      "Learning rate: 0.00001\n",
      "Epoch 6528: train loss 0.0352\n",
      "Learning rate: 0.00001\n",
      "Epoch 6530: train loss 0.0352\n",
      "Learning rate: 0.00001\n",
      "Epoch 6532: train loss 0.0352\n",
      "Learning rate: 0.00001\n",
      "Epoch 6534: train loss 0.0352\n",
      "Learning rate: 0.00001\n",
      "Epoch 6536: train loss 0.0352\n",
      "Learning rate: 0.00001\n",
      "Epoch 6538: train loss 0.0352\n",
      "Learning rate: 0.00001\n",
      "Epoch 6540: train loss 0.0352\n",
      "Learning rate: 0.00001\n",
      "Epoch 6542: train loss 0.0352\n",
      "Learning rate: 0.00001\n",
      "Epoch 6544: train loss 0.0352\n",
      "Learning rate: 0.00001\n",
      "Epoch 6546: train loss 0.0352\n",
      "Learning rate: 0.00001\n",
      "Epoch 6548: train loss 0.0352\n",
      "Learning rate: 0.00001\n",
      "Epoch 6550: train loss 0.0352\n",
      "Learning rate: 0.00001\n",
      "Epoch 6552: train loss 0.0352\n",
      "Learning rate: 0.00001\n",
      "Epoch 6554: train loss 0.0352\n",
      "Learning rate: 0.00001\n",
      "Epoch 6556: train loss 0.0352\n",
      "Learning rate: 0.00001\n",
      "Epoch 6558: train loss 0.0352\n",
      "Learning rate: 0.00001\n",
      "Epoch 6560: train loss 0.0351\n",
      "Learning rate: 0.00001\n",
      "Epoch 6562: train loss 0.0351\n",
      "Learning rate: 0.00001\n",
      "Epoch 6564: train loss 0.0351\n",
      "Learning rate: 0.00001\n",
      "Epoch 6566: train loss 0.0351\n",
      "Learning rate: 0.00001\n",
      "Epoch 6568: train loss 0.0351\n",
      "Learning rate: 0.00001\n",
      "Epoch 6570: train loss 0.0351\n",
      "Learning rate: 0.00001\n",
      "Epoch 6572: train loss 0.0351\n",
      "Learning rate: 0.00001\n",
      "Epoch 6574: train loss 0.0351\n",
      "Learning rate: 0.00001\n",
      "Epoch 6576: train loss 0.0351\n",
      "Learning rate: 0.00001\n",
      "Epoch 6578: train loss 0.0351\n",
      "Learning rate: 0.00001\n",
      "Epoch 6580: train loss 0.0351\n",
      "Learning rate: 0.00001\n",
      "Epoch 6582: train loss 0.0351\n",
      "Learning rate: 0.00001\n",
      "Epoch 6584: train loss 0.0351\n",
      "Learning rate: 0.00001\n",
      "Epoch 6586: train loss 0.0351\n",
      "Learning rate: 0.00001\n",
      "Epoch 6588: train loss 0.0351\n",
      "Learning rate: 0.00001\n",
      "Epoch 6590: train loss 0.0351\n",
      "Learning rate: 0.00001\n",
      "Epoch 6592: train loss 0.0349\n",
      "Learning rate: 0.00001\n",
      "Epoch 6594: train loss 0.0349\n",
      "Learning rate: 0.00001\n",
      "Epoch 6596: train loss 0.0349\n",
      "Learning rate: 0.00001\n",
      "Epoch 6598: train loss 0.0349\n",
      "Learning rate: 0.00001\n",
      "Epoch 6600: train loss 0.0349\n",
      "Learning rate: 0.00001\n",
      "Epoch 6602: train loss 0.0349\n",
      "Learning rate: 0.00001\n",
      "Epoch 6604: train loss 0.0349\n",
      "Learning rate: 0.00001\n",
      "Epoch 6606: train loss 0.0349\n",
      "Learning rate: 0.00001\n",
      "Epoch 6608: train loss 0.0349\n",
      "Learning rate: 0.00001\n",
      "Epoch 6610: train loss 0.0349\n",
      "Learning rate: 0.00001\n",
      "Epoch 6612: train loss 0.0349\n",
      "Learning rate: 0.00001\n",
      "Epoch 6614: train loss 0.0349\n",
      "Learning rate: 0.00001\n",
      "Epoch 6616: train loss 0.0349\n",
      "Learning rate: 0.00001\n",
      "Epoch 6618: train loss 0.0349\n",
      "Learning rate: 0.00001\n",
      "Epoch 6620: train loss 0.0349\n",
      "Learning rate: 0.00001\n",
      "Epoch 6622: train loss 0.0349\n",
      "Learning rate: 0.00001\n",
      "Epoch 6624: train loss 0.0348\n",
      "Learning rate: 0.00001\n",
      "Epoch 6626: train loss 0.0348\n",
      "Learning rate: 0.00001\n",
      "Epoch 6628: train loss 0.0348\n",
      "Learning rate: 0.00001\n",
      "Epoch 6630: train loss 0.0348\n",
      "Learning rate: 0.00001\n",
      "Epoch 6632: train loss 0.0348\n",
      "Learning rate: 0.00001\n",
      "Epoch 6634: train loss 0.0348\n",
      "Learning rate: 0.00001\n",
      "Epoch 6636: train loss 0.0348\n",
      "Learning rate: 0.00001\n",
      "Epoch 6638: train loss 0.0348\n",
      "Learning rate: 0.00001\n",
      "Epoch 6640: train loss 0.0348\n",
      "Learning rate: 0.00001\n",
      "Epoch 6642: train loss 0.0348\n",
      "Learning rate: 0.00001\n",
      "Epoch 6644: train loss 0.0348\n",
      "Learning rate: 0.00001\n",
      "Epoch 6646: train loss 0.0348\n",
      "Learning rate: 0.00001\n",
      "Epoch 6648: train loss 0.0348\n",
      "Learning rate: 0.00001\n",
      "Epoch 6650: train loss 0.0348\n",
      "Learning rate: 0.00001\n",
      "Epoch 6652: train loss 0.0348\n",
      "Learning rate: 0.00001\n",
      "Epoch 6654: train loss 0.0348\n",
      "Learning rate: 0.00001\n",
      "Epoch 6656: train loss 0.0347\n",
      "Learning rate: 0.00001\n",
      "Epoch 6658: train loss 0.0347\n",
      "Learning rate: 0.00001\n",
      "Epoch 6660: train loss 0.0347\n",
      "Learning rate: 0.00001\n",
      "Epoch 6662: train loss 0.0347\n",
      "Learning rate: 0.00001\n",
      "Epoch 6664: train loss 0.0347\n",
      "Learning rate: 0.00001\n",
      "Epoch 6666: train loss 0.0347\n",
      "Learning rate: 0.00001\n",
      "Epoch 6668: train loss 0.0347\n",
      "Learning rate: 0.00001\n",
      "Epoch 6670: train loss 0.0347\n",
      "Learning rate: 0.00001\n",
      "Epoch 6672: train loss 0.0347\n",
      "Learning rate: 0.00001\n",
      "Epoch 6674: train loss 0.0347\n",
      "Learning rate: 0.00001\n",
      "Epoch 6676: train loss 0.0347\n",
      "Learning rate: 0.00001\n",
      "Epoch 6678: train loss 0.0347\n",
      "Learning rate: 0.00001\n",
      "Epoch 6680: train loss 0.0347\n",
      "Learning rate: 0.00001\n",
      "Epoch 6682: train loss 0.0347\n",
      "Learning rate: 0.00001\n",
      "Epoch 6684: train loss 0.0347\n",
      "Learning rate: 0.00001\n",
      "Epoch 6686: train loss 0.0347\n",
      "Learning rate: 0.00001\n",
      "Epoch 6688: train loss 0.0346\n",
      "Learning rate: 0.00001\n",
      "Epoch 6690: train loss 0.0346\n",
      "Learning rate: 0.00001\n",
      "Epoch 6692: train loss 0.0346\n",
      "Learning rate: 0.00001\n",
      "Epoch 6694: train loss 0.0346\n",
      "Learning rate: 0.00001\n",
      "Epoch 6696: train loss 0.0346\n",
      "Learning rate: 0.00001\n",
      "Epoch 6698: train loss 0.0346\n",
      "Learning rate: 0.00001\n",
      "Epoch 6700: train loss 0.0346\n",
      "Learning rate: 0.00001\n",
      "Epoch 6702: train loss 0.0346\n",
      "Learning rate: 0.00001\n",
      "Epoch 6704: train loss 0.0346\n",
      "Learning rate: 0.00001\n",
      "Epoch 6706: train loss 0.0346\n",
      "Learning rate: 0.00001\n",
      "Epoch 6708: train loss 0.0346\n",
      "Learning rate: 0.00001\n",
      "Epoch 6710: train loss 0.0346\n",
      "Learning rate: 0.00001\n",
      "Epoch 6712: train loss 0.0346\n",
      "Learning rate: 0.00001\n",
      "Epoch 6714: train loss 0.0346\n",
      "Learning rate: 0.00001\n",
      "Epoch 6716: train loss 0.0346\n",
      "Learning rate: 0.00001\n",
      "Epoch 6718: train loss 0.0346\n",
      "Learning rate: 0.00001\n",
      "Epoch 6720: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6722: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6724: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6726: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6728: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6730: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6732: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6734: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6736: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6738: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6740: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6742: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6744: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6746: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6748: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6750: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6752: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6754: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6756: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6758: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6760: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6762: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6764: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6766: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6768: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6770: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6772: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6774: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6776: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6778: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6780: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6782: train loss 0.0344\n",
      "Learning rate: 0.00001\n",
      "Epoch 6784: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6786: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6788: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6790: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6792: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6794: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6796: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6798: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6800: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6802: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6804: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6806: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6808: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6810: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6812: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6814: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6816: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6818: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6820: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6822: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6824: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6826: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6828: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6830: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6832: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6834: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6836: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6838: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6840: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6842: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6844: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6846: train loss 0.0342\n",
      "Learning rate: 0.00001\n",
      "Epoch 6848: train loss 0.0341\n",
      "Learning rate: 0.00001\n",
      "Epoch 6850: train loss 0.0341\n",
      "Learning rate: 0.00001\n",
      "Epoch 6852: train loss 0.0341\n",
      "Learning rate: 0.00001\n",
      "Epoch 6854: train loss 0.0341\n",
      "Learning rate: 0.00001\n",
      "Epoch 6856: train loss 0.0341\n",
      "Learning rate: 0.00001\n",
      "Epoch 6858: train loss 0.0341\n",
      "Learning rate: 0.00001\n",
      "Epoch 6860: train loss 0.0341\n",
      "Learning rate: 0.00001\n",
      "Epoch 6862: train loss 0.0341\n",
      "Learning rate: 0.00001\n",
      "Epoch 6864: train loss 0.0341\n",
      "Learning rate: 0.00001\n",
      "Epoch 6866: train loss 0.0341\n",
      "Learning rate: 0.00001\n",
      "Epoch 6868: train loss 0.0341\n",
      "Learning rate: 0.00001\n",
      "Epoch 6870: train loss 0.0341\n",
      "Learning rate: 0.00001\n",
      "Epoch 6872: train loss 0.0341\n",
      "Learning rate: 0.00001\n",
      "Epoch 6874: train loss 0.0341\n",
      "Learning rate: 0.00001\n",
      "Epoch 6876: train loss 0.0341\n",
      "Learning rate: 0.00001\n",
      "Epoch 6878: train loss 0.0341\n",
      "Learning rate: 0.00001\n",
      "Epoch 6880: train loss 0.0340\n",
      "Learning rate: 0.00001\n",
      "Epoch 6882: train loss 0.0340\n",
      "Learning rate: 0.00001\n",
      "Epoch 6884: train loss 0.0340\n",
      "Learning rate: 0.00001\n",
      "Epoch 6886: train loss 0.0340\n",
      "Learning rate: 0.00001\n",
      "Epoch 6888: train loss 0.0340\n",
      "Learning rate: 0.00001\n",
      "Epoch 6890: train loss 0.0340\n",
      "Learning rate: 0.00001\n",
      "Epoch 6892: train loss 0.0340\n",
      "Learning rate: 0.00001\n",
      "Epoch 6894: train loss 0.0340\n",
      "Learning rate: 0.00001\n",
      "Epoch 6896: train loss 0.0340\n",
      "Learning rate: 0.00001\n",
      "Epoch 6898: train loss 0.0340\n",
      "Learning rate: 0.00001\n",
      "Epoch 6900: train loss 0.0340\n",
      "Learning rate: 0.00001\n",
      "Epoch 6902: train loss 0.0340\n",
      "Learning rate: 0.00001\n",
      "Epoch 6904: train loss 0.0340\n",
      "Learning rate: 0.00001\n",
      "Epoch 6906: train loss 0.0340\n",
      "Learning rate: 0.00001\n",
      "Epoch 6908: train loss 0.0340\n",
      "Learning rate: 0.00001\n",
      "Epoch 6910: train loss 0.0340\n",
      "Learning rate: 0.00001\n",
      "Epoch 6912: train loss 0.0339\n",
      "Learning rate: 0.00001\n",
      "Epoch 6914: train loss 0.0339\n",
      "Learning rate: 0.00001\n",
      "Epoch 6916: train loss 0.0339\n",
      "Learning rate: 0.00001\n",
      "Epoch 6918: train loss 0.0339\n",
      "Learning rate: 0.00001\n",
      "Epoch 6920: train loss 0.0339\n",
      "Learning rate: 0.00001\n",
      "Epoch 6922: train loss 0.0339\n",
      "Learning rate: 0.00001\n",
      "Epoch 6924: train loss 0.0339\n",
      "Learning rate: 0.00001\n",
      "Epoch 6926: train loss 0.0339\n",
      "Learning rate: 0.00001\n",
      "Epoch 6928: train loss 0.0339\n",
      "Learning rate: 0.00001\n",
      "Epoch 6930: train loss 0.0339\n",
      "Learning rate: 0.00001\n",
      "Epoch 6932: train loss 0.0339\n",
      "Learning rate: 0.00001\n",
      "Epoch 6934: train loss 0.0339\n",
      "Learning rate: 0.00001\n",
      "Epoch 6936: train loss 0.0339\n",
      "Learning rate: 0.00001\n",
      "Epoch 6938: train loss 0.0339\n",
      "Learning rate: 0.00001\n",
      "Epoch 6940: train loss 0.0339\n",
      "Learning rate: 0.00001\n",
      "Epoch 6942: train loss 0.0339\n",
      "Learning rate: 0.00001\n",
      "Epoch 6944: train loss 0.0333\n",
      "Learning rate: 0.00001\n",
      "Epoch 6946: train loss 0.0333\n",
      "Learning rate: 0.00001\n",
      "Epoch 6948: train loss 0.0333\n",
      "Learning rate: 0.00001\n",
      "Epoch 6950: train loss 0.0333\n",
      "Learning rate: 0.00001\n",
      "Epoch 6952: train loss 0.0333\n",
      "Learning rate: 0.00001\n",
      "Epoch 6954: train loss 0.0333\n",
      "Learning rate: 0.00001\n",
      "Epoch 6956: train loss 0.0333\n",
      "Learning rate: 0.00001\n",
      "Epoch 6958: train loss 0.0333\n",
      "Learning rate: 0.00001\n",
      "Epoch 6960: train loss 0.0333\n",
      "Learning rate: 0.00001\n",
      "Epoch 6962: train loss 0.0333\n",
      "Learning rate: 0.00001\n",
      "Epoch 6964: train loss 0.0333\n",
      "Learning rate: 0.00001\n",
      "Epoch 6966: train loss 0.0333\n",
      "Learning rate: 0.00001\n",
      "Epoch 6968: train loss 0.0333\n",
      "Learning rate: 0.00001\n",
      "Epoch 6970: train loss 0.0333\n",
      "Learning rate: 0.00001\n",
      "Epoch 6972: train loss 0.0333\n",
      "Learning rate: 0.00001\n",
      "Epoch 6974: train loss 0.0333\n",
      "Learning rate: 0.00001\n",
      "Epoch 6976: train loss 0.0320\n",
      "Learning rate: 0.00001\n",
      "Epoch 6978: train loss 0.0320\n",
      "Learning rate: 0.00001\n",
      "Epoch 6980: train loss 0.0320\n",
      "Learning rate: 0.00001\n",
      "Epoch 6982: train loss 0.0320\n",
      "Learning rate: 0.00001\n",
      "Epoch 6984: train loss 0.0320\n",
      "Learning rate: 0.00001\n",
      "Epoch 6986: train loss 0.0320\n",
      "Learning rate: 0.00001\n",
      "Epoch 6988: train loss 0.0320\n",
      "Learning rate: 0.00001\n",
      "Epoch 6990: train loss 0.0320\n",
      "Learning rate: 0.00001\n",
      "Epoch 6992: train loss 0.0320\n",
      "Learning rate: 0.00001\n",
      "Epoch 6994: train loss 0.0320\n",
      "Learning rate: 0.00001\n",
      "Epoch 6996: train loss 0.0320\n",
      "Learning rate: 0.00001\n",
      "Epoch 6998: train loss 0.0320\n",
      "Learning rate: 0.00001\n",
      "Epoch 7000: train loss 0.0320\n",
      "Learning rate: 0.00001\n",
      "Epoch 7002: train loss 0.0320\n",
      "Learning rate: 0.00001\n",
      "Epoch 7004: train loss 0.0320\n",
      "Learning rate: 0.00001\n",
      "Epoch 7006: train loss 0.0320\n",
      "Learning rate: 0.00001\n",
      "Epoch 7008: train loss 0.0317\n",
      "Learning rate: 0.00001\n",
      "Epoch 7010: train loss 0.0317\n",
      "Learning rate: 0.00001\n",
      "Epoch 7012: train loss 0.0317\n",
      "Learning rate: 0.00001\n",
      "Epoch 7014: train loss 0.0317\n",
      "Learning rate: 0.00001\n",
      "Epoch 7016: train loss 0.0317\n",
      "Learning rate: 0.00001\n",
      "Epoch 7018: train loss 0.0317\n",
      "Learning rate: 0.00001\n",
      "Epoch 7020: train loss 0.0317\n",
      "Learning rate: 0.00001\n",
      "Epoch 7022: train loss 0.0317\n",
      "Learning rate: 0.00001\n",
      "Epoch 7024: train loss 0.0317\n",
      "Learning rate: 0.00001\n",
      "Epoch 7026: train loss 0.0317\n",
      "Learning rate: 0.00001\n",
      "Epoch 7028: train loss 0.0317\n",
      "Learning rate: 0.00001\n",
      "Epoch 7030: train loss 0.0317\n",
      "Learning rate: 0.00001\n",
      "Epoch 7032: train loss 0.0317\n",
      "Learning rate: 0.00001\n",
      "Epoch 7034: train loss 0.0317\n",
      "Learning rate: 0.00001\n",
      "Epoch 7036: train loss 0.0317\n",
      "Learning rate: 0.00001\n",
      "Epoch 7038: train loss 0.0317\n",
      "Learning rate: 0.00001\n",
      "Epoch 7040: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7042: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7044: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7046: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7048: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7050: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7052: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7054: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7056: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7058: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7060: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7062: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7064: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7066: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7068: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7070: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7072: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7074: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7076: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7078: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7080: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7082: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7084: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7086: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7088: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7090: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7092: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7094: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7096: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7098: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7100: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7102: train loss 0.0315\n",
      "Learning rate: 0.00001\n",
      "Epoch 7104: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7106: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7108: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7110: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7112: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7114: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7116: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7118: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7120: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7122: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7124: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7126: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7128: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7130: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7132: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7134: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7136: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7138: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7140: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7142: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7144: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7146: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7148: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7150: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7152: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7154: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7156: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7158: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7160: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7162: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7164: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7166: train loss 0.0313\n",
      "Learning rate: 0.00001\n",
      "Epoch 7168: train loss 0.0312\n",
      "Learning rate: 0.00001\n",
      "Epoch 7170: train loss 0.0312\n",
      "Learning rate: 0.00001\n",
      "Epoch 7172: train loss 0.0312\n",
      "Learning rate: 0.00001\n",
      "Epoch 7174: train loss 0.0312\n",
      "Learning rate: 0.00001\n",
      "Epoch 7176: train loss 0.0312\n",
      "Learning rate: 0.00001\n",
      "Epoch 7178: train loss 0.0312\n",
      "Learning rate: 0.00001\n",
      "Epoch 7180: train loss 0.0312\n",
      "Learning rate: 0.00001\n",
      "Epoch 7182: train loss 0.0312\n",
      "Learning rate: 0.00001\n",
      "Epoch 7184: train loss 0.0312\n",
      "Learning rate: 0.00001\n",
      "Epoch 7186: train loss 0.0312\n",
      "Learning rate: 0.00001\n",
      "Epoch 7188: train loss 0.0312\n",
      "Learning rate: 0.00001\n",
      "Epoch 7190: train loss 0.0312\n",
      "Learning rate: 0.00001\n",
      "Epoch 7192: train loss 0.0312\n",
      "Learning rate: 0.00001\n",
      "Epoch 7194: train loss 0.0312\n",
      "Learning rate: 0.00001\n",
      "Epoch 7196: train loss 0.0312\n",
      "Learning rate: 0.00001\n",
      "Epoch 7198: train loss 0.0312\n",
      "Learning rate: 0.00001\n",
      "Epoch 7200: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7202: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7204: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7206: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7208: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7210: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7212: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7214: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7216: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7218: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7220: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7222: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7224: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7226: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7228: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7230: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7232: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7234: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7236: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7238: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7240: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7242: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7244: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7246: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7248: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7250: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7252: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7254: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7256: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7258: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7260: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7262: train loss 0.0311\n",
      "Learning rate: 0.00001\n",
      "Epoch 7264: train loss 0.0310\n",
      "Learning rate: 0.00001\n",
      "Epoch 7266: train loss 0.0310\n",
      "Learning rate: 0.00001\n",
      "Epoch 7268: train loss 0.0310\n",
      "Learning rate: 0.00001\n",
      "Epoch 7270: train loss 0.0310\n",
      "Learning rate: 0.00001\n",
      "Epoch 7272: train loss 0.0310\n",
      "Learning rate: 0.00001\n",
      "Epoch 7274: train loss 0.0310\n",
      "Learning rate: 0.00001\n",
      "Epoch 7276: train loss 0.0310\n",
      "Learning rate: 0.00001\n",
      "Epoch 7278: train loss 0.0310\n",
      "Learning rate: 0.00001\n",
      "Epoch 7280: train loss 0.0310\n",
      "Learning rate: 0.00001\n",
      "Epoch 7282: train loss 0.0310\n",
      "Learning rate: 0.00001\n",
      "Epoch 7284: train loss 0.0310\n",
      "Learning rate: 0.00001\n",
      "Epoch 7286: train loss 0.0310\n",
      "Learning rate: 0.00001\n",
      "Epoch 7288: train loss 0.0310\n",
      "Learning rate: 0.00001\n",
      "Epoch 7290: train loss 0.0310\n",
      "Learning rate: 0.00001\n",
      "Epoch 7292: train loss 0.0310\n",
      "Learning rate: 0.00001\n",
      "Epoch 7294: train loss 0.0310\n",
      "Learning rate: 0.00001\n",
      "Epoch 7296: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7298: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7300: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7302: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7304: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7306: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7308: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7310: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7312: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7314: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7316: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7318: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7320: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7322: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7324: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7326: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7328: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7330: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7332: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7334: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7336: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7338: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7340: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7342: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7344: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7346: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7348: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7350: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7352: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7354: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7356: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7358: train loss 0.0309\n",
      "Learning rate: 0.00001\n",
      "Epoch 7360: train loss 0.0308\n",
      "Learning rate: 0.00001\n",
      "Epoch 7362: train loss 0.0308\n",
      "Learning rate: 0.00001\n",
      "Epoch 7364: train loss 0.0308\n",
      "Learning rate: 0.00001\n",
      "Epoch 7366: train loss 0.0308\n",
      "Learning rate: 0.00001\n",
      "Epoch 7368: train loss 0.0308\n",
      "Learning rate: 0.00001\n",
      "Epoch 7370: train loss 0.0308\n",
      "Learning rate: 0.00001\n",
      "Epoch 7372: train loss 0.0308\n",
      "Learning rate: 0.00001\n",
      "Epoch 7374: train loss 0.0308\n",
      "Learning rate: 0.00001\n",
      "Epoch 7376: train loss 0.0308\n",
      "Learning rate: 0.00001\n",
      "Epoch 7378: train loss 0.0308\n",
      "Learning rate: 0.00001\n",
      "Epoch 7380: train loss 0.0308\n",
      "Learning rate: 0.00001\n",
      "Epoch 7382: train loss 0.0308\n",
      "Learning rate: 0.00001\n",
      "Epoch 7384: train loss 0.0308\n",
      "Learning rate: 0.00001\n",
      "Epoch 7386: train loss 0.0308\n",
      "Learning rate: 0.00001\n",
      "Epoch 7388: train loss 0.0308\n",
      "Learning rate: 0.00001\n",
      "Epoch 7390: train loss 0.0308\n",
      "Learning rate: 0.00001\n",
      "Epoch 7392: train loss 0.0307\n",
      "Learning rate: 0.00001\n",
      "Epoch 7394: train loss 0.0307\n",
      "Learning rate: 0.00001\n",
      "Epoch 7396: train loss 0.0307\n",
      "Learning rate: 0.00001\n",
      "Epoch 7398: train loss 0.0307\n",
      "Learning rate: 0.00001\n",
      "Epoch 7400: train loss 0.0307\n",
      "Learning rate: 0.00001\n",
      "Epoch 7402: train loss 0.0307\n",
      "Learning rate: 0.00001\n",
      "Epoch 7404: train loss 0.0307\n",
      "Learning rate: 0.00001\n",
      "Epoch 7406: train loss 0.0307\n",
      "Learning rate: 0.00001\n",
      "Epoch 7408: train loss 0.0307\n",
      "Learning rate: 0.00001\n",
      "Epoch 7410: train loss 0.0307\n",
      "Learning rate: 0.00001\n",
      "Epoch 7412: train loss 0.0307\n",
      "Learning rate: 0.00001\n",
      "Epoch 7414: train loss 0.0307\n",
      "Learning rate: 0.00001\n",
      "Epoch 7416: train loss 0.0307\n",
      "Learning rate: 0.00001\n",
      "Epoch 7418: train loss 0.0307\n",
      "Learning rate: 0.00001\n",
      "Epoch 7420: train loss 0.0307\n",
      "Learning rate: 0.00001\n",
      "Epoch 7422: train loss 0.0307\n",
      "Learning rate: 0.00001\n",
      "Epoch 7424: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7426: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7428: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7430: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7432: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7434: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7436: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7438: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7440: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7442: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7444: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7446: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7448: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7450: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7452: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7454: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7456: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7458: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7460: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7462: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7464: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7466: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7468: train loss 0.0306\n",
      "Learning rate: 0.00001\n",
      "Epoch 7470: train loss 0.0306\n",
      "Learning rate: 0.00000\n",
      "Epoch 7472: train loss 0.0306\n",
      "Learning rate: 0.00000\n",
      "Epoch 7474: train loss 0.0306\n",
      "Learning rate: 0.00000\n",
      "Epoch 7476: train loss 0.0306\n",
      "Learning rate: 0.00000\n",
      "Epoch 7478: train loss 0.0306\n",
      "Learning rate: 0.00000\n",
      "Epoch 7480: train loss 0.0306\n",
      "Learning rate: 0.00000\n",
      "Epoch 7482: train loss 0.0306\n",
      "Learning rate: 0.00000\n",
      "Epoch 7484: train loss 0.0306\n",
      "Learning rate: 0.00000\n",
      "Epoch 7486: train loss 0.0306\n",
      "Learning rate: 0.00000\n",
      "Epoch 7488: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7490: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7492: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7494: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7496: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7498: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7500: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7502: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7504: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7506: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7508: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7510: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7512: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7514: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7516: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7518: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7520: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7522: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7524: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7526: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7528: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7530: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7532: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7534: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7536: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7538: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7540: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7542: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7544: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7546: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7548: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7550: train loss 0.0305\n",
      "Learning rate: 0.00000\n",
      "Epoch 7552: train loss 0.0304\n",
      "Learning rate: 0.00000\n",
      "Epoch 7554: train loss 0.0304\n",
      "Learning rate: 0.00000\n",
      "Epoch 7556: train loss 0.0304\n",
      "Learning rate: 0.00000\n",
      "Epoch 7558: train loss 0.0304\n",
      "Learning rate: 0.00000\n",
      "Epoch 7560: train loss 0.0304\n",
      "Learning rate: 0.00000\n",
      "Epoch 7562: train loss 0.0304\n",
      "Learning rate: 0.00000\n",
      "Epoch 7564: train loss 0.0304\n",
      "Learning rate: 0.00000\n",
      "Epoch 7566: train loss 0.0304\n",
      "Learning rate: 0.00000\n",
      "Epoch 7568: train loss 0.0304\n",
      "Learning rate: 0.00000\n",
      "Epoch 7570: train loss 0.0304\n",
      "Learning rate: 0.00000\n",
      "Epoch 7572: train loss 0.0304\n",
      "Learning rate: 0.00000\n",
      "Epoch 7574: train loss 0.0304\n",
      "Learning rate: 0.00000\n",
      "Epoch 7576: train loss 0.0304\n",
      "Learning rate: 0.00000\n",
      "Epoch 7578: train loss 0.0304\n",
      "Learning rate: 0.00000\n",
      "Epoch 7580: train loss 0.0304\n",
      "Learning rate: 0.00000\n",
      "Epoch 7582: train loss 0.0304\n",
      "Learning rate: 0.00000\n",
      "Epoch 7584: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7586: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7588: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7590: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7592: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7594: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7596: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7598: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7600: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7602: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7604: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7606: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7608: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7610: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7612: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7614: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7616: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7618: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7620: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7622: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7624: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7626: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7628: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7630: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7632: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7634: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7636: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7638: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7640: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7642: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7644: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7646: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7648: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7650: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7652: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7654: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7656: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7658: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7660: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7662: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7664: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7666: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7668: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7670: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7672: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7674: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7676: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7678: train loss 0.0303\n",
      "Learning rate: 0.00000\n",
      "Epoch 7680: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7682: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7684: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7686: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7688: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7690: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7692: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7694: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7696: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7698: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7700: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7702: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7704: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7706: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7708: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7710: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7712: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7714: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7716: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7718: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7720: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7722: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7724: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7726: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7728: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7730: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7732: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7734: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7736: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7738: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7740: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7742: train loss 0.0302\n",
      "Learning rate: 0.00000\n",
      "Epoch 7744: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7746: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7748: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7750: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7752: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7754: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7756: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7758: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7760: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7762: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7764: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7766: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7768: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7770: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7772: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7774: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7776: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7778: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7780: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7782: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7784: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7786: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7788: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7790: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7792: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7794: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7796: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7798: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7800: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7802: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7804: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7806: train loss 0.0301\n",
      "Learning rate: 0.00000\n",
      "Epoch 7808: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7810: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7812: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7814: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7816: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7818: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7820: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7822: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7824: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7826: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7828: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7830: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7832: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7834: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7836: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7838: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7840: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7842: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7844: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7846: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7848: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7850: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7852: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7854: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7856: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7858: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7860: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7862: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7864: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7866: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7868: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7870: train loss 0.0300\n",
      "Learning rate: 0.00000\n",
      "Epoch 7872: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7874: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7876: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7878: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7880: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7882: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7884: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7886: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7888: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7890: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7892: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7894: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7896: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7898: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7900: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7902: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7904: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7906: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7908: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7910: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7912: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7914: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7916: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7918: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7920: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7922: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7924: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7926: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7928: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7930: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7932: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7934: train loss 0.0299\n",
      "Learning rate: 0.00000\n",
      "Epoch 7936: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7938: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7940: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7942: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7944: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7946: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7948: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7950: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7952: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7954: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7956: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7958: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7960: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7962: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7964: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7966: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7968: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7970: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7972: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7974: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7976: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7978: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7980: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7982: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7984: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7986: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7988: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7990: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7992: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7994: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7996: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 7998: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 8000: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 8002: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 8004: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 8006: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 8008: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 8010: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 8012: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 8014: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 8016: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 8018: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 8020: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 8022: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 8024: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 8026: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 8028: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 8030: train loss 0.0298\n",
      "Learning rate: 0.00000\n",
      "Epoch 8032: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8034: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8036: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8038: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8040: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8042: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8044: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8046: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8048: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8050: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8052: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8054: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8056: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8058: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8060: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8062: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8064: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8066: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8068: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8070: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8072: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8074: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8076: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8078: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8080: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8082: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8084: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8086: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8088: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8090: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8092: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8094: train loss 0.0297\n",
      "Learning rate: 0.00000\n",
      "Epoch 8096: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8098: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8100: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8102: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8104: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8106: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8108: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8110: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8112: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8114: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8116: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8118: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8120: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8122: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8124: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8126: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8128: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8130: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8132: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8134: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8136: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8138: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8140: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8142: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8144: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8146: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8148: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8150: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8152: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8154: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8156: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8158: train loss 0.0296\n",
      "Learning rate: 0.00000\n",
      "Epoch 8160: train loss 0.0295\n",
      "Learning rate: 0.00000\n",
      "Epoch 8162: train loss 0.0295\n",
      "Learning rate: 0.00000\n",
      "Epoch 8164: train loss 0.0295\n",
      "Learning rate: 0.00000\n",
      "Epoch 8166: train loss 0.0295\n",
      "Learning rate: 0.00000\n",
      "Epoch 8168: train loss 0.0295\n",
      "Learning rate: 0.00000\n",
      "Epoch 8170: train loss 0.0295\n",
      "Learning rate: 0.00000\n",
      "Epoch 8172: train loss 0.0295\n",
      "Learning rate: 0.00000\n",
      "Epoch 8174: train loss 0.0295\n",
      "Learning rate: 0.00000\n",
      "Epoch 8176: train loss 0.0295\n",
      "Learning rate: 0.00000\n",
      "Epoch 8178: train loss 0.0295\n",
      "Learning rate: 0.00000\n",
      "Epoch 8180: train loss 0.0295\n",
      "Learning rate: 0.00000\n",
      "Epoch 8182: train loss 0.0295\n",
      "Learning rate: 0.00000\n",
      "Epoch 8184: train loss 0.0295\n",
      "Learning rate: 0.00000\n",
      "Epoch 8186: train loss 0.0295\n",
      "Learning rate: 0.00000\n",
      "Epoch 8188: train loss 0.0295\n",
      "Learning rate: 0.00000\n",
      "Epoch 8190: train loss 0.0295\n",
      "Learning rate: 0.00000\n",
      "Epoch 8192: train loss 0.0293\n",
      "Learning rate: 0.00000\n",
      "Epoch 8194: train loss 0.0293\n",
      "Learning rate: 0.00000\n",
      "Epoch 8196: train loss 0.0293\n",
      "Learning rate: 0.00000\n",
      "Epoch 8198: train loss 0.0293\n",
      "Learning rate: 0.00000\n",
      "Epoch 8200: train loss 0.0293\n",
      "Learning rate: 0.00000\n",
      "Epoch 8202: train loss 0.0293\n",
      "Learning rate: 0.00000\n",
      "Epoch 8204: train loss 0.0293\n",
      "Learning rate: 0.00000\n",
      "Epoch 8206: train loss 0.0293\n",
      "Learning rate: 0.00000\n",
      "Epoch 8208: train loss 0.0293\n",
      "Learning rate: 0.00000\n",
      "Epoch 8210: train loss 0.0293\n",
      "Learning rate: 0.00000\n",
      "Epoch 8212: train loss 0.0293\n",
      "Learning rate: 0.00000\n",
      "Epoch 8214: train loss 0.0293\n",
      "Learning rate: 0.00000\n",
      "Epoch 8216: train loss 0.0293\n",
      "Learning rate: 0.00000\n",
      "Epoch 8218: train loss 0.0293\n",
      "Learning rate: 0.00000\n",
      "Epoch 8220: train loss 0.0293\n",
      "Learning rate: 0.00000\n",
      "Epoch 8222: train loss 0.0293\n",
      "Learning rate: 0.00000\n",
      "Epoch 8224: train loss 0.0291\n",
      "Learning rate: 0.00000\n",
      "Epoch 8226: train loss 0.0291\n",
      "Learning rate: 0.00000\n",
      "Epoch 8228: train loss 0.0291\n",
      "Learning rate: 0.00000\n",
      "Epoch 8230: train loss 0.0291\n",
      "Learning rate: 0.00000\n",
      "Epoch 8232: train loss 0.0291\n",
      "Learning rate: 0.00000\n",
      "Epoch 8234: train loss 0.0291\n",
      "Learning rate: 0.00000\n",
      "Epoch 8236: train loss 0.0291\n",
      "Learning rate: 0.00000\n",
      "Epoch 8238: train loss 0.0291\n",
      "Learning rate: 0.00000\n",
      "Epoch 8240: train loss 0.0291\n",
      "Learning rate: 0.00000\n",
      "Epoch 8242: train loss 0.0291\n",
      "Learning rate: 0.00000\n",
      "Epoch 8244: train loss 0.0291\n",
      "Learning rate: 0.00000\n",
      "Epoch 8246: train loss 0.0291\n",
      "Learning rate: 0.00000\n",
      "Epoch 8248: train loss 0.0291\n",
      "Learning rate: 0.00000\n",
      "Epoch 8250: train loss 0.0291\n",
      "Learning rate: 0.00000\n",
      "Epoch 8252: train loss 0.0291\n",
      "Learning rate: 0.00000\n",
      "Epoch 8254: train loss 0.0291\n",
      "Learning rate: 0.00000\n",
      "Epoch 8256: train loss 0.0287\n",
      "Learning rate: 0.00000\n",
      "Epoch 8258: train loss 0.0287\n",
      "Learning rate: 0.00000\n",
      "Epoch 8260: train loss 0.0287\n",
      "Learning rate: 0.00000\n",
      "Epoch 8262: train loss 0.0287\n",
      "Learning rate: 0.00000\n",
      "Epoch 8264: train loss 0.0287\n",
      "Learning rate: 0.00000\n",
      "Epoch 8266: train loss 0.0287\n",
      "Learning rate: 0.00000\n",
      "Epoch 8268: train loss 0.0287\n",
      "Learning rate: 0.00000\n",
      "Epoch 8270: train loss 0.0287\n",
      "Learning rate: 0.00000\n",
      "Epoch 8272: train loss 0.0287\n",
      "Learning rate: 0.00000\n",
      "Epoch 8274: train loss 0.0287\n",
      "Learning rate: 0.00000\n",
      "Epoch 8276: train loss 0.0287\n",
      "Learning rate: 0.00000\n",
      "Epoch 8278: train loss 0.0287\n",
      "Learning rate: 0.00000\n",
      "Epoch 8280: train loss 0.0287\n",
      "Learning rate: 0.00000\n",
      "Epoch 8282: train loss 0.0287\n",
      "Learning rate: 0.00000\n",
      "Epoch 8284: train loss 0.0287\n",
      "Learning rate: 0.00000\n",
      "Epoch 8286: train loss 0.0287\n",
      "Learning rate: 0.00000\n",
      "Epoch 8288: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8290: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8292: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8294: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8296: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8298: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8300: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8302: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8304: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8306: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8308: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8310: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8312: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8314: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8316: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8318: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8320: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8322: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8324: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8326: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8328: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8330: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8332: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8334: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8336: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8338: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8340: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8342: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8344: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8346: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8348: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8350: train loss 0.0283\n",
      "Learning rate: 0.00000\n",
      "Epoch 8352: train loss 0.0282\n",
      "Learning rate: 0.00000\n",
      "Epoch 8354: train loss 0.0282\n",
      "Learning rate: 0.00000\n",
      "Epoch 8356: train loss 0.0282\n",
      "Learning rate: 0.00000\n",
      "Epoch 8358: train loss 0.0282\n",
      "Learning rate: 0.00000\n",
      "Epoch 8360: train loss 0.0282\n",
      "Learning rate: 0.00000\n",
      "Epoch 8362: train loss 0.0282\n",
      "Learning rate: 0.00000\n",
      "Epoch 8364: train loss 0.0282\n",
      "Learning rate: 0.00000\n",
      "Epoch 8366: train loss 0.0282\n",
      "Learning rate: 0.00000\n",
      "Epoch 8368: train loss 0.0282\n",
      "Learning rate: 0.00000\n",
      "Epoch 8370: train loss 0.0282\n",
      "Learning rate: 0.00000\n",
      "Epoch 8372: train loss 0.0282\n",
      "Learning rate: 0.00000\n",
      "Epoch 8374: train loss 0.0282\n",
      "Learning rate: 0.00000\n",
      "Epoch 8376: train loss 0.0282\n",
      "Learning rate: 0.00000\n",
      "Epoch 8378: train loss 0.0282\n",
      "Learning rate: 0.00000\n",
      "Epoch 8380: train loss 0.0282\n",
      "Learning rate: 0.00000\n",
      "Epoch 8382: train loss 0.0282\n",
      "Learning rate: 0.00000\n",
      "Epoch 8384: train loss 0.0280\n",
      "Learning rate: 0.00000\n",
      "Epoch 8386: train loss 0.0280\n",
      "Learning rate: 0.00000\n",
      "Epoch 8388: train loss 0.0280\n",
      "Learning rate: 0.00000\n",
      "Epoch 8390: train loss 0.0280\n",
      "Learning rate: 0.00000\n",
      "Epoch 8392: train loss 0.0280\n",
      "Learning rate: 0.00000\n",
      "Epoch 8394: train loss 0.0280\n",
      "Learning rate: 0.00000\n",
      "Epoch 8396: train loss 0.0280\n",
      "Learning rate: 0.00000\n",
      "Epoch 8398: train loss 0.0280\n",
      "Learning rate: 0.00000\n",
      "Epoch 8400: train loss 0.0280\n",
      "Learning rate: 0.00000\n",
      "Epoch 8402: train loss 0.0280\n",
      "Learning rate: 0.00000\n",
      "Epoch 8404: train loss 0.0280\n",
      "Learning rate: 0.00000\n",
      "Epoch 8406: train loss 0.0280\n",
      "Learning rate: 0.00000\n",
      "Epoch 8408: train loss 0.0280\n",
      "Learning rate: 0.00000\n",
      "Epoch 8410: train loss 0.0280\n",
      "Learning rate: 0.00000\n",
      "Epoch 8412: train loss 0.0280\n",
      "Learning rate: 0.00000\n",
      "Epoch 8414: train loss 0.0280\n",
      "Learning rate: 0.00000\n",
      "Epoch 8416: train loss 0.0278\n",
      "Learning rate: 0.00000\n",
      "Epoch 8418: train loss 0.0278\n",
      "Learning rate: 0.00000\n",
      "Epoch 8420: train loss 0.0278\n",
      "Learning rate: 0.00000\n",
      "Epoch 8422: train loss 0.0278\n",
      "Learning rate: 0.00000\n",
      "Epoch 8424: train loss 0.0278\n",
      "Learning rate: 0.00000\n",
      "Epoch 8426: train loss 0.0278\n",
      "Learning rate: 0.00000\n",
      "Epoch 8428: train loss 0.0278\n",
      "Learning rate: 0.00000\n",
      "Epoch 8430: train loss 0.0278\n",
      "Learning rate: 0.00000\n",
      "Epoch 8432: train loss 0.0278\n",
      "Learning rate: 0.00000\n",
      "Epoch 8434: train loss 0.0278\n",
      "Learning rate: 0.00000\n",
      "Epoch 8436: train loss 0.0278\n",
      "Learning rate: 0.00000\n",
      "Epoch 8438: train loss 0.0278\n",
      "Learning rate: 0.00000\n",
      "Epoch 8440: train loss 0.0278\n",
      "Learning rate: 0.00000\n",
      "Epoch 8442: train loss 0.0278\n",
      "Learning rate: 0.00000\n",
      "Epoch 8444: train loss 0.0278\n",
      "Learning rate: 0.00000\n",
      "Epoch 8446: train loss 0.0278\n",
      "Learning rate: 0.00000\n",
      "Epoch 8448: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8450: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8452: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8454: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8456: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8458: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8460: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8462: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8464: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8466: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8468: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8470: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8472: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8474: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8476: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8478: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8480: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8482: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8484: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8486: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8488: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8490: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8492: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8494: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8496: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8498: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8500: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8502: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8504: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8506: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8508: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8510: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8512: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8514: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8516: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8518: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8520: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8522: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8524: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8526: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8528: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8530: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8532: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8534: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8536: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8538: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8540: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8542: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8544: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8546: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8548: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8550: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8552: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8554: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8556: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8558: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8560: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8562: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8564: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8566: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8568: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8570: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8572: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8574: train loss 0.0276\n",
      "Learning rate: 0.00000\n",
      "Epoch 8576: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8578: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8580: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8582: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8584: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8586: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8588: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8590: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8592: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8594: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8596: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8598: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8600: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8602: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8604: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8606: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8608: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8610: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8612: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8614: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8616: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8618: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8620: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8622: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8624: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8626: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8628: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8630: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8632: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8634: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8636: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8638: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8640: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8642: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8644: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8646: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8648: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8650: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8652: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8654: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8656: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8658: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8660: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8662: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8664: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8666: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8668: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8670: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8672: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8674: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8676: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8678: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8680: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8682: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8684: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8686: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8688: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8690: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8692: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8694: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8696: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8698: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8700: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8702: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8704: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8706: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8708: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8710: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8712: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8714: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8716: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8718: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8720: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8722: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8724: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8726: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8728: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8730: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8732: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8734: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8736: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8738: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8740: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8742: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8744: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8746: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8748: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8750: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8752: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8754: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8756: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8758: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8760: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8762: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8764: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8766: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8768: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8770: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8772: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8774: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8776: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8778: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8780: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8782: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8784: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8786: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8788: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8790: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8792: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8794: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8796: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8798: train loss 0.0275\n",
      "Learning rate: 0.00000\n",
      "Epoch 8800: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8802: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8804: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8806: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8808: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8810: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8812: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8814: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8816: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8818: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8820: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8822: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8824: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8826: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8828: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8830: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8832: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8834: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8836: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8838: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8840: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8842: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8844: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8846: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8848: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8850: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8852: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8854: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8856: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8858: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8860: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8862: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8864: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8866: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8868: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8870: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8872: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8874: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8876: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8878: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8880: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8882: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8884: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8886: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8888: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8890: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8892: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8894: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8896: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8898: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8900: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8902: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8904: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8906: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8908: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8910: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8912: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8914: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8916: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8918: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8920: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8922: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8924: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8926: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8928: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8930: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8932: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8934: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8936: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8938: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8940: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8942: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8944: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8946: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8948: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8950: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8952: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8954: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8956: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8958: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8960: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8962: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8964: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8966: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8968: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8970: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8972: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8974: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8976: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8978: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8980: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8982: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8984: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8986: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8988: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8990: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8992: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8994: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8996: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 8998: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9000: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9002: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9004: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9006: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9008: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9010: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9012: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9014: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9016: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9018: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9020: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9022: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9024: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9026: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9028: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9030: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9032: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9034: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9036: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9038: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9040: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9042: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9044: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9046: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9048: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9050: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9052: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9054: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9056: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9058: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9060: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9062: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9064: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9066: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9068: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9070: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9072: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9074: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9076: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9078: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9080: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9082: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9084: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9086: train loss 0.0274\n",
      "Learning rate: 0.00000\n",
      "Epoch 9088: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9090: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9092: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9094: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9096: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9098: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9100: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9102: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9104: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9106: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9108: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9110: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9112: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9114: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9116: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9118: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9120: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9122: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9124: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9126: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9128: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9130: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9132: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9134: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9136: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9138: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9140: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9142: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9144: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9146: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9148: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9150: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9152: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9154: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9156: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9158: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9160: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9162: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9164: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9166: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9168: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9170: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9172: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9174: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9176: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9178: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9180: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9182: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9184: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9186: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9188: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9190: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9192: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9194: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9196: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9198: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9200: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9202: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9204: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9206: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9208: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9210: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9212: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9214: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9216: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9218: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9220: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9222: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9224: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9226: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9228: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9230: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9232: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9234: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9236: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9238: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9240: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9242: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9244: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9246: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9248: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9250: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9252: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9254: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9256: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9258: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9260: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9262: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9264: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9266: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9268: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9270: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9272: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9274: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9276: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9278: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9280: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9282: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9284: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9286: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9288: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9290: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9292: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9294: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9296: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9298: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9300: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9302: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9304: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9306: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9308: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9310: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9312: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9314: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9316: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9318: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9320: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9322: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9324: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9326: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9328: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9330: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9332: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9334: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9336: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9338: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9340: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9342: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9344: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9346: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9348: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9350: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9352: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9354: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9356: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9358: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9360: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9362: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9364: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9366: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9368: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9370: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9372: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9374: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9376: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9378: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9380: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9382: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9384: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9386: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9388: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9390: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9392: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9394: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9396: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9398: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9400: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9402: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9404: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9406: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9408: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9410: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9412: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9414: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9416: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9418: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9420: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9422: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9424: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9426: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9428: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9430: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9432: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9434: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9436: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9438: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9440: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9442: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9444: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9446: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9448: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9450: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9452: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9454: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9456: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9458: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9460: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9462: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9464: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9466: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9468: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9470: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9472: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9474: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9476: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9478: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9480: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9482: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9484: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9486: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9488: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9490: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9492: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9494: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9496: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9498: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9500: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9502: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9504: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9506: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9508: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9510: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9512: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9514: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9516: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9518: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9520: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9522: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9524: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9526: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9528: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9530: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9532: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9534: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9536: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9538: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9540: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9542: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9544: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9546: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9548: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9550: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9552: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9554: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9556: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9558: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9560: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9562: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9564: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9566: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9568: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9570: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9572: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9574: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9576: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9578: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9580: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9582: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9584: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9586: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9588: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9590: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9592: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9594: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9596: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9598: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9600: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9602: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9604: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9606: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9608: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9610: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9612: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9614: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9616: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9618: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9620: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9622: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9624: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9626: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9628: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9630: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9632: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9634: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9636: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9638: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9640: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9642: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9644: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9646: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9648: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9650: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9652: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9654: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9656: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9658: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9660: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9662: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9664: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9666: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9668: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9670: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9672: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9674: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9676: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9678: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9680: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9682: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9684: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9686: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9688: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9690: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9692: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9694: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9696: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9698: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9700: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9702: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9704: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9706: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9708: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9710: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9712: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9714: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9716: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9718: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9720: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9722: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9724: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9726: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9728: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9730: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9732: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9734: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9736: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9738: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9740: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9742: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9744: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9746: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9748: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9750: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9752: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9754: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9756: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9758: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9760: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9762: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9764: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9766: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9768: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9770: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9772: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9774: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9776: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9778: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9780: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9782: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9784: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9786: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9788: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9790: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9792: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9794: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9796: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9798: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9800: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9802: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9804: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9806: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9808: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9810: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9812: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9814: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9816: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9818: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9820: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9822: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9824: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9826: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9828: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9830: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9832: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9834: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9836: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9838: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9840: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9842: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9844: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9846: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9848: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9850: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9852: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9854: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9856: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9858: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9860: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9862: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9864: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9866: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9868: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9870: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9872: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9874: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9876: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9878: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9880: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9882: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9884: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9886: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9888: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9890: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9892: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9894: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9896: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9898: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9900: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9902: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9904: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9906: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9908: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9910: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9912: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9914: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9916: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9918: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9920: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9922: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9924: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9926: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9928: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9930: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9932: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9934: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9936: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9938: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9940: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9942: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9944: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9946: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9948: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9950: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9952: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9954: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9956: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9958: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9960: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9962: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9964: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9966: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9968: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9970: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9972: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9974: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9976: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9978: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9980: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9982: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9984: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9986: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9988: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9990: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9992: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9994: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9996: train loss 0.0273\n",
      "Learning rate: 0.00000\n",
      "Epoch 9998: train loss 0.0273\n",
      "Learning rate: 0.00000\n"
     ]
    }
   ],
   "source": [
    "best_train_loss = float('inf')\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "train_loss_list = []\n",
    "\n",
    "# Move model to the correct device\n",
    "model = model.to(device)\n",
    "\n",
    "# Learning rate scheduler setup\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_iters)\n",
    "\n",
    "# Estimate loss on train only\n",
    "def estimate_loss(model):\n",
    "    model.eval()\n",
    "    losses = torch.zeros(eval_iters)\n",
    "    with torch.inference_mode():\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(\"train\")\n",
    "            X, Y = X.to(device), Y.to(device)\n",
    "            with ctx:\n",
    "                _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "    model.train()\n",
    "    return {\"train\": losses.mean()}\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(max_iters)):\n",
    "    if epoch % eval_iters == 0 and epoch != 0:\n",
    "        losses = estimate_loss(model)\n",
    "        train_loss = losses[\"train\"]\n",
    "\n",
    "        print(f\"Epoch {epoch}: train loss {train_loss:.4f}\")\n",
    "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "\n",
    "        train_loss_list.append(train_loss)\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "    # Get batch and move to device\n",
    "    X, y = get_batch(\"train\")\n",
    "    X, y = X.to(device), y.to(device)\n",
    "\n",
    "    with ctx:\n",
    "        logits, loss = model(X, y)\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    # Perform optimizer step after accumulation\n",
    "    if ((epoch + 1) % gradient_accumulation_steps == 0) or (epoch + 1 == max_iters):\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4s51hTfJQZQI"
   },
   "outputs": [],
   "source": [
    "#plot the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "3XmP4eQeQZSe",
    "outputId": "83ccffc3-2e76-417a-c47b-6d9a52f3b9cc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "train_loss_list_converted = [i.cpu().detach() for i in train_loss_list]\n",
    "validation_loss_list_converted = [i.cpu().detach() for i in validation_loss_list]\n",
    "\n",
    "plt.plot(train_loss_list_converted, 'g', label='train_loss')\n",
    "plt.xlabel(\"Steps - Every 100 epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wR8vzRdQZUj"
   },
   "outputs": [],
   "source": [
    "#run inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5bi4ajQnQZWo",
    "outputId": "9abe3922-8a8d-4702-a6c2-91c3aced5370"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the model\n",
    "model = GPT(config)  # re-create the model with same config\n",
    "device =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "best_model_params_path = \"best_model_params.pt\"\n",
    "model.load_state_dict(torch.load(best_model_params_path, map_location=torch.device(device))) # load best model states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c10NMoH0QZY5",
    "outputId": "266c893f-e856-4237-d601-60f52619384f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zeolites are materials!!!!!Govern!!!!!!!!!!!!!!! honored!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! slipping!!!!!!!!!article!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!cookie!!!!!!\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Zeolites are materials\"\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "y = model.generate(context, 200)\n",
    "print(enc.decode(y.squeeze().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dOmmLtAPUJp-",
    "outputId": "0bbf536c-0563-4914-9644-3e58ba3f1cd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Through ML, synthesis conditions were!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!405!!!!!!erred!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Through ML, synthesis conditions were\"\n",
    "context = (torch.tensor(enc.encode_ordinary(sentence)).unsqueeze(dim = 0))\n",
    "y = model.generate(context, 100)\n",
    "print(enc.decode(y.squeeze().tolist()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "158b951ececd40d78e0df5cca76d3e0a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ca30d2521da4e4b8324eb365636ac3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d7b613e94454059885788cc7fbf7f95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b07e24380423450f8197024d063877f6",
      "placeholder": "​",
      "style": "IPY_MODEL_b44f1305bf8b4c10ae8507989652d9a7",
      "value": "100%"
     }
    },
    "7aa046c30428450eaf3a280eaf5246ae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a43415a28a1641e39e8b4505bc99a2af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_158b951ececd40d78e0df5cca76d3e0a",
      "placeholder": "​",
      "style": "IPY_MODEL_7aa046c30428450eaf3a280eaf5246ae",
      "value": " 10000/10000 [17:32&lt;00:00, 10.48it/s]"
     }
    },
    "b07e24380423450f8197024d063877f6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b44f1305bf8b4c10ae8507989652d9a7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d5a0eb54f7d64563bf4e98920bdaaf78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dad97c3adf974344aad23b2f0989852e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f20ffe8779e04a45910d964632296575",
      "max": 10000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d5a0eb54f7d64563bf4e98920bdaaf78",
      "value": 10000
     }
    },
    "f20ffe8779e04a45910d964632296575": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f622839e6ebc458c8536174713d96a12": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6d7b613e94454059885788cc7fbf7f95",
       "IPY_MODEL_dad97c3adf974344aad23b2f0989852e",
       "IPY_MODEL_a43415a28a1641e39e8b4505bc99a2af"
      ],
      "layout": "IPY_MODEL_3ca30d2521da4e4b8324eb365636ac3c"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
